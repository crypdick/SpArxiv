old_file,abstract
1801_00571_DetectionAndEstimationOfPartiallyObservedDynamicalSystems.tex," In real environments, it is seldom that physical dynamical systems can be observed without detection failures and without disturbances from the background. Yet, a vast majority of the literature regarding Bayesian inference for such systems ignore these undesired effects and assume that pre-processing can be applied to remove them. To some extent, this goes against the Bayesian philosophy which promotes the integration of the different aspects of the problem into a joint formulation. However, such a formulation usually involves a precise modelling of these adverse effects as well as the setting of the corresponding parameters, which is not always feasible or realistic. In this article, we propose to use outer measures of a certain form to allow for additional flexibility in the modelling of these effects within the Bayesian paradigm. It is shown that detection and estimation of partially-observed dynamical systems can be performed with little to no knowledge about the background disturbances and with only an upper bound on the probability of detection failure. It is conformed in simulations that such an approach can compete with standard methods even when the latter are given the true parameter values. "
1801_04080_contracts.tex,"  In a continuous-time setting where a risk-averse agent controls the  drift of an output process driven by a Brownian motion, optimal  contracts are linear in the terminal output; this result is  well-known in a setting with moral hazard and -- under stronger  assumptions -- adverse selection.  We show that this result continues to hold when in addition  reservation utilities are type-dependent. This type of problem  occurs in the study of optimal compensation problems involving  competing principals. \medskip  \noindent Keywords:  Principal-agent modelling; contract design; stochastic process;  stochastic control% "
1801_03181_abstract.tex," We describe the first public data release of the Dark Energy Survey, DES DR1, consisting of reduced single-epoch images, coadded images, coadded source catalogs, and associated products and services assembled over the first three years of DES science operations. %DES DR1 is based on 345 distinct nights (August 2013 to February 2016) of optical/near-infrared imaging by the Dark Energy Camera mounted on the 4-m Blanco telescope at Cerro Tololo Inter-American Observatory in Chile. DES DR1 is based on optical/near-infrared imaging from 345 distinct nights (August 2013 to February 2016) by the Dark Energy Camera mounted on the 4-m Blanco telescope at Cerro Tololo Inter-American Observatory in Chile. We release data from the DES wide-area survey covering  $\roughly 5000 \deg^2$ of the southern Galactic cap in five broad photometric bands, $grizY$ . DES DR1 has a median delivered point-spread function of $g = \medfwhmg$ , $r = \medfwhmr$ , $i = \medfwhmi$ , $z = \medfwhmz$ , and $Y = \medfwhmy$  \asec FWHM, a photometric precision of $<1$ \% in all bands, and an astrometric precision of $\astroabs~\mas$ . % are measured relative to Gaia . %The median coadded catalog depth for \rm S/N = 10$ is $g = \maglimsnrg$ , $r = \maglimsnrr$ , $i = \maglimsnri$ , $z = \maglimsnrz$ , and $Y = \maglimsnry$  \magn. The median coadded catalog depth for a 1.95 \rm S/N = 10$ is $g = \maglimsnraperg$ , $r = \maglimsnraperr$ , $i = \maglimsnraperi$ , $z = \maglimsnraperz$ , and $Y = \maglimsnrapery$  \magn. DES DR1 includes nearly $400$ M distinct astronomical objects detected in $,000$ coadd tiles of size  $0.534 \deg^2$ produced from $,000$ individual exposures. Benchmark galaxy and stellar samples contain $\rm M$ and $\rm M$ objects, respectively, following a basic object quality selection. These data are accessible through a range of interfaces, including query web clients, image cutout servers, jupyter notebooks, and an interactive coadd image visualization tool. DES DR1 constitutes the largest photometric data set to date at the achieved depth and photometric precision. %The \nexposures individual exposures comprising DES DR1 were acquired with the Dark Energy Camera mounted on the Blanco 4-m telescope at Cerro Tololo Inter-American Observatory over 315 nights between August 2013 and February 2016. %based on the first three years of full science operations. %which are intended to support general astronomical investigations by the community. % The Dark Energy Survey (DES) %DES is a Stage II dark energy project designed to understand the origin of cosmic acceleration. %DES performs %Approximately XX survey-quality exposures were acquired with the Dark Energy Camera mounted on the %DES is designed to study the origin of cosmic acceleration using the Dark Energy Camera mounted on the Blanco 4-m telescope at Cerro Tololo Inter-American Observatory.  %by DES, a $\sim5000$ deg $^2$ imaging survey of the south Galactic cap in five optical and near-IR bands ( $grizY$ ) ultimately reaching a depth of $\sim$ 24th magnitude in the $i$ -band. % %The \nexposures individual exposures comprising DES DR1 were acquired with the Dark Energy Camera mounted on the Blanco 4-m telescope at Cerro Tololo Inter-American Observatory over 315 nights between August 2013 and February 2016. %The median delivered seeing in the $riz$ bands was ${\rm FWHM} \sim 0\farcs.9$ . %A photometric precision of $<1$ \% and astrometric precision of $\sim150$ ~mas are measured relative to Gaia . %The median coadded \magauto $S/N = 10$ depth in the $grizY$ bands is \maglimsnrg, \maglimsnrr, \maglimsnri, \maglimsnrz, and \maglimsnry \magn. %DR1 includes nearly $400$ M distinct astronomical objects detected in \roughly 10,000 coadded images produced from the individual exposures. %These data products are accessible through a range of interfaces, including an SQL web client, image cutout server, and interactive footprint search location and coadd image visualization tools. %Contemporaneously, DES performs a deep, time-domain survey in four optical bands ($g,r,i,z$ ) over $\sim 27$ square-degree. % I don't think we should emphasize the SN survey in the abstract if we are not releasing any of the associated data. %DES exposures are processed nightly with an evolving data reduction pipeline and evaluated for image quality to determine if they need to be retaken. %Difference imaging and transient source detection is also performed nightly. % Also, we are not releasing any difference imaging products, only the single-epoch detections in the incremental release of summer 2018. %On a yearly basis, DES exposures are reprocessed with a refined pipeline and coadded to maximize imaging depth. %DR1 includes $\sim400$ M distinct astronomical objects detected in the coadded images. %The median delivered $i$ -band seeing is $\sim0.9 \asec$ , photometric calibration of $\sim1$ \%, and astrometric precision of 150 mas. %Here we describe the DES image processing pipeline in support of DES science, as a reference for users of archival DES data, and as a guide for future astronomical surveys. "
1801_00778_main.tex, Quantum algorithm plays a notable role in solving linear systems of equations with an exponential speedup over the classical algorithm. Here we demonstrate Gaussian elimination method for solving system of equations by using the well known Grover's quantum search algorithm. The elimination method mainly involves elementary row operations which can be performed by applying particular matrices that can be obtained from Grover's algorithm. We explicitly illustrate the whole process by taking a simple example consisting of a set of equations.
1801_05270_Lane_manuscript.tex," We demonstrate in situ gate-tunable acoustoelectric transport in exfoliated monolayer graphene by measuring the voltage created as high-frequency surface acoustic waves dynamically drive charge carriers in the graphene. We employ a flip-chip device configuration to conduct acoustoelectric measurements while simultaneously controlling the graphene carrier density with a metal-oxide back-gate. At high carrier density we observe dependence of the acoustoelectric signal on the sign of the graphene charge carriers, while at low densities we observe anomalous sign reversals of the acoustoelectrically generated voltage. We attribute these anomalous sign reversals to spatially heterogeneous conduction in the vicinity of charge neutrality. "
1801_04076_deep.tex," Deep learning and convolutional neural networks (CNN) have been intensively used in many image processing topics during last years. As far as steganalysis is concerned, the use of CNN allows reaching the state-of-the-art results. The performances of such networks often rely on the size of their learning database. An obvious preliminary assumption could be considering that “the bigger a database is, the better the results are”. However, it appears that cautions have to be taken when increasing the database size if one desire to improve the classification accuracy i.e. enhance the steganalysis efficiency. To our knowledge, no study has been performed on the enrichment impact of a learning database on the steganalysis performance. What kind of images can be added to the initial learning set? What are the sensitive criteria: the camera models used for acquiring the images, the treatments applied to the images, the cameras proportions in the database, etc? This article continues the work carried out in a previous paper in submission Yedroudj2018_Net , and explores the ways to improve the performances of CNN. It aims at studying the effects of “base augmentation” on the performance of steganalysis using a CNN. We present the results of this study using various experimental protocols and various databases to define the good practices in base augmentation for steganalysis. "
1801_10103_FIACPO.tex," 	We investigate systems of the form $A^tg:g\in\G,t\in[0,L]\$ where $A \in B(\HH)$ is a normal operator in a separable Hilbert space $\HH$ , $\G\subset \HH$ is a countable set, and $L$ is a positive real number. Although the main goal of this work is to study the frame properties of  $A^tg:g\in\G,t\in[0,L]\$ , as intermediate steps, we explore the completeness and Bessel properties of such systems from a theoretical perspective, which are of interest by themselves. Beside the theoretical appeal of investigating such systems, their connections to dynamical and mobile sampling make them fundamental for understanding and solving several major problems in  engineering and science. "
1801_10583_ms.tex," Due to the liberalization of markets, the change in the energy mix and the surrounding energy laws, electricity research is a dynamically altering field with steadily changing challenges. One challenge especially for investment decisions is to provide reliable short to mid-term forecasts despite high variation in the time series of electricity prices. This paper tackles this issue in a promising and novel approach. By utilizing  combining the high precision of econometric autoregressive models in the short-run with  and the expectations of market participants reflected in future prices for the short- and mid-run we show that the forecasting performance can be vastly increased while maintaining hourly precision. We investigate the day-ahead electricity price of the EPEX Spot for Germany and Austria and setup a model which incorporates the Phelix future of the EEX for Germany and Austria. The model can be considered as an AR24-X model with one distinct model for each hour of the day. We are able to show that future data contains relevant price information for future time periods of the day-ahead electricity price. We show that relying only on deterministic external regressors can provide stability for forecast horizons of multiple weeks.  By implementing a fast and efficient lasso estimation approach we demonstrate that our model can outperform several other models in the literature. "
1801_08388_top.tex," Non-Rigid structure from motion ( ), is a long standing and central problem in computer vision, allowing us to obtain 3D information from multiple images when the scene is dynamic. A main issue regarding the further development of this important computer vision topic, is the lack of high quality data sets. We here address this issue by presenting of data set compiled for this purpose, which is made publicly available, and considerably larger than previous state of the art. To validate the applicability of this data set, and provide and investigation into the state of the art of  , including potential directions forward, we here present a benchmark and a scrupulous evaluation using this data set. This benchmark evaluates 16 different methods with available code, which we argue reasonably spans the state of the art in  . We also hope, that the presented and public data set and evaluation, will provide benchmark tools for further development in this field. "
1801_07197.tex," We show that, for a finitely generated residually finite group $\Gamma$ , the word $[x_1, \ldots, x_k]$ is a probabilistic identity of $\Gamma$ if and only if $\Gamma$ is virtually nilpotent of class less than $k$ . Related results, generalizations and problems are also discussed. "
1801_02665_Symbolic_relative_entropy_in_equalities-involved_heartbeats.tex," Symbolic relative entropy, an efficient nonlinear complexity parameter measuring probabilistic divergences of symbolic sequences, is proposed in our nonlinear dynamics analysis of heart rates considering equal states. Equalities are not rare in discrete heartbeats because of the limits of resolution of signals collection, and more importantly equal states contain underlying important cardiac regulation information which is neglected by some chaotic deterministic parameters and temporal asymmetric measurements. The relative entropy of symbolization associated with equal states has satisfied nonlinear dynamics complexity detections in heartbeats and shows advantages to some nonlinear dynamics parameters without considering equalities. Researches on cardiac activities suggest the highest probabilistic divergence of the healthy young heart rates and highlight the facts that heart diseases and aging reduce the nonlinear dynamical complexity of heart rates. "
1801_03844_main.tex,"  In this paper, we explore the usage of Word Embedding semantic resources for Information Retrieval (IR) task. This embedding, produced by a shallow neural network, have been shown to catch semantic similarities between words article6 . Hence, our goal is to enhance IR Language Models by addressing the term mismatch problem. To do so, we applied the model presented in the paper Integrating and Evaluating Neural Word Embedding in Information Retrieval by article2 that proposes to estimate the translation probability of a Translation Language Model using the cosine similarity between Word Embedding. The results we obtained so far did not show a statistically significant improvement compared to classical Language Model.   Information Retrieval, Language Model , Word Embedding "
1801_09964_paper-v2.tex," 1cm Due to the absence of well-defined concepts of time and energy in background independent systems, formulating statistical equilibrium in such settings remains an open issue. Even more so in the full quantum gravity context, not based on any of the usual spacetime notions but on non-spatiotemporal degrees of freedom. In this paper, after having clarified different general notions of statistical equilibrium, on which different construction procedures can be based, we focus on the group field theory formalism for quantum gravity, whose technical features prove advantageous to the task. We use the operatorial formulation of group field theory to define its statistical mechanical framework, and, based on this, we construct three concrete examples of Gibbs states. The first is a Gibbs state with respect to a geometric volume operator, which is shown to support condensation to a low-spin phase. This state is not based on a pre-defined flow and its construction is via Jaynes' entropy maximisation principle. The second are Gibbs states encoding structural equilibrium with respect to internal translations on the GFT base manifold, and defined via the KMS condition. The third are Gibbs states encoding relational equilibrium with respect to a clock Hamiltonian, obtained by deparametrization with respect to coupled scalar matter fields. "
1801_09094_DDM_layered_7.tex,"  We develop a non-overlapping domain decomposition method (DDM) for the solution of quasi-periodic scalar transmission problems in layered media. Our approach relies on robust boundary-integral equation formulations of Robin-to-Robin (RtR) maps throughout the frequency spectrum, including at Wood, or cutoff, frequencies.  We overcome the obstacle of non-convergent quasi-periodic Green functions at these frequencies by incorporating newly introduced shifted quasi-periodic Green functions. Using the latter in the definition of our quasi-periodic boundary-integral operators leads to rigorously stable computations of RtR operators. We develop Nystr \""om discretizations of the RtR maps that rely on trigonometric interpolation, singularity resolution, and fast convergent windowed quasi-periodic Green functions. We solve the tridiagonal DDM system via recursive Schur complements and we establish rigorously that this procedure is always completed successfully. We present a variety of numerical results concerning Wood frequencies in two and three dimensions as well as large numbers of layers.  % \newline   Keywords : Helmholtz transmission problem, domain decomposition, periodic layered media, lattice sum.\\    AMS subject classifications :  65N38, 35J05, 65T40,65F08 "
1801_02969_Iterative_Learning_Economic_MPC_1-column_.tex," 		An iterative learning based economic model predictive controller (ILEMPC) is proposed for repetitive tasks in this paper. Compared with existing works, the initial feasible trajectory of the proposed ILEMPC is not restricted to be convergent to an equilibrium so it can handle various types of control objectives: stabilization, tracking a periodic trajectory and even pure economic optimization. The controller can learn from the previous closed-loop trajectory, resulting in a performance which is guaranteed to be no worse than the previous one. Under some standard assumptions in model predictive control, we show that recursive feasibility is ensured. Furthermore, for stabilization problem, the convergence of each learned trajectory and the learning process are established provided the initial trajectory is convergent. Numerical examples show that the proposed control strategy works well for different types of control tasks and systems. 		 	"
1801_07000_ImprovingPFforHDproblemsUsingArtificialProcessNoise.tex,"         		The particle filter is one of the most successful methods for state inference and identification of general non-linear and non-Gaussian models. However, standard particle filters suffer from degeneracy of the particle weights for high-dimensional problems. We propose a method for improving the performance of the particle filter for certain challenging state space models, with implications for high-dimensional inference. First we approximate the model by adding artificial process noise in an additional state update, then we design a proposal that combines the standard and the locally optimal proposal. This results in a bias-variance trade-off, where adding more noise reduces the variance of the estimate but increases the model bias. The performance of the proposed method is evaluated on a linear Gaussian state space model and on the non-linear Lorenz'96 model. For both models we observe a significant improvement in performance over the standard particle filter. 	"
1801_07756_main.tex," In recent years, the use of deep learning algorithms has become increasingly more prominent. Within the field of electromyography-based gesture recognition however, deep learning algorithms are seldom employed. This is due in part to the large quantity of data required for the network to train on. The data sparsity arises from the fact that it would take an unreasonable amount of time for a single person to generate tens of thousands of examples for training such algorithms. In this paper, two datasets are recorded with the Myo Armband (Thalmic Labs), a low-cost, low-sampling rate (200 Hz ), 8-channel, consumer-grade, dry electrode sEMG armband. These datasets, referred to as the pre-training and evaluation dataset, are comprised of 19 and 17 able-bodied participants respectively. A convolutional network (ConvNet) is augmented with transfer learning techniques to leverage inter-user data from the first dataset, alleviating the burden imposed on a single individual to generate a vast quantity of training data for sEMG-based gesture recognition. This transfer learning scheme is shown to outperform the current state-of-the-art in gesture recognition achieving an average accuracy of 98.31 \% for 7 hand/wrist gestures over 17 able-bodied participants. Finally, a use-case study of eight able-bodied participants is presented to evaluate the impact of feedback on the degradation accuracy normally experienced from a classifier over time.  "
1801_01269_manuscript.tex," When two planar atomic membranes are placed within the van der Waals distance, the charge and heat transport across the interface are coupled by the rules of momentum conservation and structural commensurability, lead to outstanding thermoelectric properties. Here we show that an effective 'inter-layer phonon drag' determines the Seebeck coefficient ( $S$ ) across the van der Waals gap formed in twisted bilayer graphene (tBLG). The cross-plane thermovoltage which is nonmonotonic in both temperature and density, is generated through scattering of electrons by the out-of-plane layer breathing (ZO $^{'}$ /ZA $_{2}$ ) phonon modes and differs dramatically from the expected Landauer-Buttiker formalism in conventional tunnel junctions. The Tunability of cross-plane seebeck effect in van der Waals junctions may be valuable in creating a new genre of versatile thermoelectric systems with layered solids "
1801_04448_main.tex," Recently, it was shown both theoretically and experimentally that certain three-dimensional (3D) materials have Dirac points in the Brillouin zone, thus being 3D analogs of graphene. Moreover, it was suggested that under specific conditions a pair of adatoms placed inside a three-dimensional Dirac semimetal may lead to the formation of an unusual antibonding state. However, in any realistic system vibrational degrees of freedom play an important role. In this work we address the influence of phonons on characteristic features of (anti)bonding state, and discuss how these results can be tested experimentally via local probing, namely inelastic electron tunneling spectroscopy that is known to be linked to $d^2I/dV^2$ curve obtained in STM measurements. "
1801_01040_main-submit.tex,"\abstract{ Interplanetary (IP) shocks are known to be accelerators of energetic charged particles observed in-situ in the heliosphere. However, the acceleration of near-relativistic electrons by shocks in the interplanetary medium is often questioned. On 9 August 2011 a Corotating Interaction Region (CIR) passed STEREO B (STB) that resulted in a flux increase in the electron and ion channels of the Solar Electron and Proton Telescope (SEPT). Because electron measurements in the few~keV to several 100~keV range rely on the so-called magnet foil technique, which is utilized by SEPT, ions can contribute to the electron channels.}"
1801_07430_Cache-NOMA_CCNC2018_test_version.tex," 	Non-Orthogonal Multiple Access (NOMA) and caching are two proposed approaches to increase the capacity of future 5G wireless systems. Typically in NOMA systems, signals at the receiver are decoded using successive interference cancellation in order to achieve capacity in multi-user systems. The leveraging of caching in the physical layer to further improve on the benefits of NOMA is investigated, which is termed cache-aided NOMA. Specific attention is given to the caching cases where the users with weaker channel conditions possess a cache of the information requested by a user with a stronger channel condition. The probability that any of the users is in outage for any of the rates required for this NOMA system, defined as the ""union-outage,"" is derived for the case of fixed-power allocation, and the power allocation strategy that minimizes the union-outage probability is derived. Simulation results confirm the analytical results, which demonstrate the benefits of cache-aided NOMA on reducing the union-outages probability.   "
1801_01928_main.tex," Tensor Train decomposition is used across many branches of machine learning, but until now it lacked an implementation with GPU support, batch processing, automatic differentiation, and versatile functionality for Riemannian optimization framework, which takes in account the underlying manifold structure in order to construct efficient optimization methods. In this work, we propose a library that aims to fix it and makes machine learning papers that rely on Tensor Train decomposition easier to implement. The library includes $92\%$ test coverage, examples, and API reference documentation. "
1801_09870_main.tex," We propose a new method to efficiently compute load-flows (the steady-state of the power-grid for given productions, consumptions and grid topology), substituting conventional simulators based on differential equation solvers. We use a deep feed-forward neural network trained with load-flows precomputed by simulation. Our architecture permits to train a network on so-called ``n-1'' problems, in which load flows are evaluated for every possible line disconnection, then generalize to ``n-2'' problems without re-training (a clear advantage because of the combinatorial nature of the problem). To that end, we developed a technique bearing similarity with ``dropout'', which we named ``guided dropout''. "
1801_09774_main.tex," We present a psychoacoustically enhanced cost function to balance network complexity and perceptual performance of deep neural networks for speech denoising. While training the network, we utilize perceptual weights added to the ordinary mean-squared error to emphasize contribution from frequency bins which are most audible while ignoring error from inaudible bins. To generate the weights, we employ psychoacoustic models to compute the global masking threshold from the clean speech spectra. We then evaluate the speech denoising performance of our perceptually guided neural network by using both objective and perceptual sound quality metrics, testing on various network structures ranging from shallow and narrow ones to deep and wide ones. The experimental results showcase our method as a valid approach for infusing perceptual significance to deep neural network operations. In particular, the more perceptually sensible enhancement in performance seen by simple neural network topologies proves that the proposed method can lead to resource-efficient speech denoising implementations in small devices without degrading the perceived signal fidelity. "
1801_09179.tex," A particular case of the Hindman--Galvin--Glazer theorem states that, for every partition of an infinite abelian group $G$ into two cells, there will be an infinite $X\subseteq G$ such that the set of its finite sums $x_1+\cdots+x_n \mid n\in\mathbb N\wedge x_1,\ldots,x_n\in X\text{ are distinct\}$ is monochromatic. It is known that the same statement is false, in a very strong sense, if one attempts to obtain an uncountable (rather than just infinite) $X$ . On the other hand, a recent result of Komj \'ath states that, for partitions into uncountably many cells, it is possible to obtain monochromatic sets of the form $\fs(X)$ , for $X$ of some prescribed finite size, when working with sufficiently large Boolean groups. In this paper, we provide a generalization of Komj \'ath's result, and we show that, in a sense, this generalization is the strongest possible. "
1801_01024_NA4_versao_7.tex," We study solutions for the Klein--Gordon equation with vector and scalar potentials of the Coulomb types under the influence of non-inertial effects in the space-time of topological defects. We also investigate a quantum particle described by the Klein--Gordon oscillator in the background space-time generated by a string. An important result obtained is that the non-inertial effects restrict the physical region of the space-time where the particle can be placed. In addition, we show that these potentials can form bound states for the relativistic wave equation equation in this kind of background. "
1801_07753_aanda.tex,{}{}{}{}
1801_02635_paper_draft.tex," We present the first rest-frame UV population study of 17 heavily reddened, high-luminosity (E(B-V) $_{QSO}\gtrsim$ 0.5; L $_{bol}>$ 10 $^{46}$ ergs $^{-1}$ ) broad-line quasars at $1.5 < z < 2.7$ . We combine the first year of deep, optical, ground-based observations from the Dark Energy Survey (DES) with the near infrared VISTA Hemisphere Survey (VHS) and UKIDSS Large Area Survey (ULAS) data, from which the reddened quasars were initially identified. We demonstrate that the significant dust reddening towards the quasar in our sample allows host galaxy emission to be detected at the rest-frame UV wavelengths probed by the DES photometry. By exploiting this reddening effect, we disentangle the quasar emission from that of the host galaxy via spectral energy distribution (SED) fitting. We find evidence for a relatively unobscured, star-forming host galaxy in at least ten quasars, with a further three quasars exhibiting emission consistent with either star formation or scattered light. From the rest-frame UV emission, we derive instantaneous, dust-corrected star formation rates (SFRs) in the range 25 < SFR $_{UV}$ < 365 M $_{\odot}$ yr $^{-1}$ , with an average SFR $_{UV}$ = 130 $\pm$ 95 M $_{\odot}$ yr $^{-1}$ . We find a broad correlation between SFR $_{UV}$ and the bolometric quasar luminosity. Overall, our results show evidence for coeval star formation and black hole accretion occurring in luminous, reddened quasars at the peak epoch of galaxy formation.   "
1801_08203_Complete_Classification_of_Discrete_specializations_of_the_Burau_representation_of_B3.tex," This classification is found by analyzing the action of a normal subgroup of $B_3$ as hyperbolic isometries. This paper gives an example of an unfaithful specialization of the Burau representation on $B_4$ that is faithful when restricted to $B_3$ , as well as examples of unfaithful specializations of $B_3$ . "
1801_06152_BRITEproceedingsMMunoz.tex," All known Galactic Of?p stars have been shown to host strong, organized, magnetic fields. Recently, five Of?p stars have been discovered in the Magellanic Clouds. They posses photometric Naze and spectroscopic Walborn variability compatible with the Oblique Rotator Model (ORM). However, their magnetic fields have yet to be directly detected. We have developed an algorithm allowing for the synthesis of photometric observables based on the Analytic Dynamical Magnetosphere (ADM) model of Owocki . We apply our model to OGLE photometry in order to constrain their magnetic geometries and and surface dipole strengths. We predict that the field strengths for some of these candidate extra-Galactic magnetic stars may be within the detection limits of the FORS2 instrument. "
1801_01837_ms.tex," We present high-precision timing data over timespans of up to 11 years for 45 millisecond pulsars observed as part of the North American Nanohertz Observatory for Gravitational Waves (NANOGrav) project, aimed at detecting and characterizing low-frequency gravitational waves. The pulsars were observed with the Arecibo Observatory and/or the Green Bank Telescope at frequencies ranging from 327 MHz to 2.3 GHz. Most pulsars were observed with approximately monthly cadence, with six high--timing-precision pulsars observed weekly, and all were observed at widely separated frequencies at each observing epoch in order to fit for time-variable dispersion delays. We describe our methods for data processing, time-of-arrival (TOA) calculation, and the implementation of a new, automated method for removing outlier TOAs. We fit a timing model for each pulsar which includes spin, astrometric, and, if necessary, binary parameters, in addition to time-variable dispersion delays and parameters that quantify pulse-profile evolution with frequency. The new timing solutions provide three new parallax measurements, two new Shapiro-delay measurements, and two new measurements of large orbital-period variations. We fit models that characterize sources of noise for each pulsar. We find that 11 pulsars show significant red noise, with generally smaller spectral indices than typically measured for non-recycled pulsars, possibly suggesting a different origin. Future papers will use these data to constrain or detect the signatures of gravitational-wave signals.  "
1801_04916_ms.tex," This paper is a follow-up work about the artificial ecosystem model: number soup (Liu and Sumpter, J. Royal Soc. Interface , 2017). It elaborates more details about this model and points out future directions. "
1801_05281_search-high-z_15jan.tex," Ultra-steep spectrum (USS) radio sources are good tracers of powerful radio galaxies at $z > 2$ . Identification of even a single bright radio galaxy at $z > 6$ can be used to detect redshifted 21cm absorption due to neutral hydrogen in the intervening IGM. Here we describe a new sample of high-redshift radio galaxy (HzRG) candidates constructed from the TGSS ADR1 survey at 150 MHz. We employ USS selection ( $\alpha \le -1.3$ ) in $\sim10000$ square degrees, in combination with strict size selection and non-detections in all-sky optical and infrared surveys. We apply flux density cuts that probe a unique parameter space in flux density ( $50 < S_{150} < 200$ mJy) to build a sample of 32 HzRG candidates. Follow-up Karl G. Jansky Very Large Array (VLA) observations at 1.4 GHz with an average beam size of $1.3$ arcseconds ( $''$ ) revealed $\sim 48\%$ of sources to have a single radio component. P-band (370 MHz) imaging of 17 of these sources revealed a flattening radio SED for ten sources at low frequencies, which is expected from compact HzRGs. Two of our sources lie in fields where deeper multi-wavelength photometry and ancillary radio data are available and for one of these we find a best-fit photo-z of $4.8 \pm 2.0$ . The other source has $z_{phot}=1.4 \pm 0.1$ and a small angular size ( $3.7''$ ), which could be associated with an obscured star forming galaxy or with a `dead' elliptical. One USS radio source not part of the HzRG sample but observed with the VLA nonetheless is revealed to be a candidate giant radio galaxy with a host galaxy photo-z of $1.8\pm0.5$ , indicating a size of 875 kpc. "
1801_08638.tex," We study representations of the meromorphic open-string vertex algebra (MOSVAs hereafter) defined in H-MOSVA , a noncommutative generalization of vertex (operator) algebra. We start by recalling the definition of a MOSVA $V$ and left $V$ -modules in H-MOSVA . Then we define right $V$ -modules and $V$ -bimodules that reflect the noncommutative nature of $V$ . When $V$ satisfies a condition on the order of poles of the correlation function (which we call pole-order condition), we prove that the rationality of products of two vertex operators implies the rationality of products of any numbers of vertex operators. Also, the rationality of iterates of any numbers of vertex operators is established, and is used to construct the opposite MOSVA $V^{op}$ of $V$ . It is proved here that right (resp. left) $V$ -modules are equivalent to left (resp. right) $V^{op}$ -modules. Using this equivalence, we prove that if $V$ and a grading-restricted left $V$ -module $W$ is endowed with a M \""obius structure, then the graded dual $W'$ of $W$ is a right $V$ -module. This proof is the only place in this paper that needs the grading-restriction condition. Also, this result is generalized to not-grading-restricted modules under a strong pole-order condition that is satisfied by all existing examples of MOSVAs and modules. "
1801_07058_poincare.tex," We propose a general strategy to derive null-homotopy operators for differential complexes based on the Bernstein-Gelfand-Gelfand (BGG) construction and properties of the de Rham complex. Focusing on the elasticity complex, we derive path integral operators $P$ for elasticity satisfying $DP+PD=id$ and $P^{2}=0$ , where the differential operators $D$ correspond to the linearized deformation, the linearized curvature and the divergence, respectively. As a special case, we rederive the classical Ces a ro-Volterra path integral for strain tensors satisfying the Saint Venant compatibility condition, using the path integral for scalar potentials and techniques from homological algebra. The known path-independence of the Ces a ro-Volterra path integral can thus be seen as a result of the corresponding property of differential 1-forms.  {In general we derive path integral formulas in the presence of defects.} "
1801_07887_paper.tex," When using active learning, smaller batch sizes are typically more efficient from a learning efficiency perspective. However, in practice due to speed and human annotator considerations, the use of larger batch sizes is necessary. While past work has shown that larger batch sizes decrease learning efficiency from a learning curve perspective, it remains an open question how batch size impacts methods for stopping active learning. We find that large batch sizes degrade the performance of a leading stopping method over and above the degradation that results from reduced learning efficiency. We analyze this degradation and find that it can be mitigated by changing the window size parameter of how many past iterations of learning are taken into account when making the stopping decision. We find that when using larger batch sizes, stopping methods are more effective when smaller window sizes are used. "
1801_01394_TrexTheory.tex," The TREX is a recently introduced approach to sparse linear regression. In contrast to most well-known approaches to penalized regression, the TREX can be formulated without the use of tuning parameters.  %Recent work has shown that the TREX can be efficiently computed despite being based on a non-convex optimization problem. In this paper, we establish the first known prediction error bounds for the TREX. Additionally, we introduce extensions of the TREX to a more general class of penalties, and we provide a bound on the prediction error in this generalized setting. These results deepen the understanding of TREX from a theoretical perspective and provide new insights into penalized regression in general. TREX \and high-dimensional regression \and tuning parameters \and oracle inequalities  62J07 "
1801_08553_NPDDE_v9.tex," We explore cosmological constraints on the sum of the three active neutrino masses $\mnu$ in the context of dynamical dark energy (DDE) models with equation of state (EoS) parametrized as a function of redshift $z$ by  $w(z)=w_0+w_a\,z/(1+z)$ , and satisfying $w(z)\geq-1$ for all $z$ . We make use of Cosmic Microwave Background data from the Planck satellite, Baryon Acoustic Oscillations measurements, and Supernovae Ia luminosity distance measurements, and perform a Bayesian analysis. We show that, within these models, the bounds on $\mnu$  do not degrade with respect to those obtained in the $\lcdm$ case; in fact the bounds are slightly tighter, despite the enlarged parameter space. We explain our results based on the observation that, for fixed choices of $w_0\,,w_a$ such that $w(z)\geq-1$ (but not $w=-1$ for all $z$ ), the upper limit on $\mnu$ is tighter than the $\lcdm$ limit because of the well-known degeneracy between $w$ and $M_{\nu}$ . The Bayesian analysis we have carried out then integrates over the possible values of $w_0$ - $w_a$ such that $w(z)\geq-1$ , all of which correspond to tighter limits on $M_\nu$ than the $\lcdm$ limit. We find a 95 \% confidence level (C.L.) upper bound of $\mnu<0.13\,\eV$ . This bound can be compared with $\mnu<0.16\,\eV$ at 95 \%~C.L., obtained within the $\lcdm$ model, and $\mnu<0.41\,\eV$ at 95 \%~C.L., obtained in a DDE model with arbitrary EoS (which allows values of $w < -1$ ). Contrary to the results derived for DDE models with arbitrary EoS, we find that a dark energy component with $w(z)\geq-1$ is unable to alleviate the tension between high-redshift observables and direct measurements of the Hubble constant $H_0$ . Finally, in light of the results of this analysis, we also discuss the implications for DDE models of a possible determination of the neutrino mass hierarchy by laboratory searches. "
1801_08829_ieee_shonan.tex," Symbol emergence through a robot's own interactive exploration of the world without human intervention has been investigated now for several decades. However, methods that enable a machine to form symbol systems in a robust bottom-up manner are still missing. Clearly this shows that we still do not have an appropriate computational understanding that explains symbol emergence in biological and artificial systems. Over the years it became more and more clear that symbol emergence has to be posed as a multi-facetted problem. Therefore, we will first review the history of the symbol emergence problem in different fields showing their mutual relations. Then we will describe recent work and approaches to solve this problem with the aim of providing an integrative and comprehensive overview of symbol emergence for future research. "
1801_03164_abstract.tex," In this paper, we present \system, a parallel anomaly dataset generator. We discuss its design and provide brief experimental results demonstrating its usefulness in improving the classification correctness of LSTM-AD, a state-of-the-art anomaly detection model. "
1801_09613_Euler_problem_MartynchukDullinEfstathiouWaalkens.tex,"  The problem of two fixed centers was introduced by Euler as early as in 1760. It plays an important role both in celestial mechanics and in the microscopic world. In the present paper we study the spatial  problem in the case of arbitrary (both positive and negative) strengths of the centers. Combining techniques from scattering theory and Liouville integrability,  we show that this spatial problem has  topologically non-trivial scattering dynamics, which we identify as scattering monodromy.  The approach that we introduce in this paper applies more generally  to scattering systems that are integrable in the Liouville sense.\\  %Our approach is based on techniques from the theory of Liouville integrable systems and scattering theory.   -2mm  \\  Keywords: Action-angle coordinates; Hamiltonian systems; Liouville integrability; Scattering map; Scattering monodromy. "
1801_02727_gapsdscuti.tex,{}{}{}{}
1801_01211_RMF-revised.tex," \bf Abstract: We study the generation of an electromagnetic current in monolayer graphene immersed in a weak perpendicular magnetic field and radiated with linearly polarized monochromatic light. Such a current emits Bremsstrahlung radiation with the same amplitude above and below the plane of the sample, in the latter case consistent with the small amount of light absorption in the material. This mechanism could be an important contribution for the reflexion of light phenomenon in graphene.   \bf Resumen: Estudiamos la generaci \'on de una corriente electromagn \'etica en una monocapa de grafeno inmersa en un campo magn \'etico d \'ebil perpendicular y radiada con luz monocrom \'atica. Esta corriente emite radiaci \'on de Bremsstrahlung con la misma amplitud por arriba y abajo del plano de la muestra, en el \'ultimo caso consistente con la peque \~na cantidad de absorci \'on de luz en el material. Este mecanismo puede ser una contribuci \'on importante para el fen \'omeno de reflexi \'on de luz en grafeno. "
1801_04867_MousleyRussell_Morse_boundary_HHG_final.tex,"\abstract{We generalize a result of Paulin on the Gromov boundary of hyperbolic groups to the Morse boundary of proper, maximal hierarchically hyperbolic spaces admitting cocompact group actions by isometries. Namely we show that if the Morse boundaries of two such spaces each contain at least three points, then the spaces are quasi-isometric if and only if there exists a 2--stable, quasi-m\""obius homeomorphism between their Morse boundaries. Our result extends a recent result of Charney--Murray, who prove such a classification for CAT(0) groups, and is new for mapping class groups and the fundamental groups of $3$--manifolds without Nil or Sol components.}"
1801_09589_main.tex," Subnetwork extraction using community detection methods is commonly used to study the brain's modular structure. Recent studies indicated that certain brain regions are known to interact with multiple subnetworks. However, most existing methods are mainly for non-overlapping subnetwork extraction. In this paper, we present an approach for overlapping brain subnetwork extraction using cliques, which we defined as co-activated node groups performing multiple tasks. We proposed a multisource subnetwork extraction approach based on the co-activated clique , which (1) uses task co-activation and task connectivity strength information for clique identification, (2) automatically detects cliques of different sizes having more neuroscientific justifications, and (3) shares the subnetwork membership, derived from a fusion of rest and task data, among the nodes within a clique for overlapping subnetwork extraction. On real data, compared to the commonly used overlapping community detection techniques, we showed that our approach improved subnetwork extraction in terms of group-level and subject-wise reproducibility. We also showed that our multisource approach identified subnetwork overlaps within brain regions that matched well with hubs defined using functional and anatomical information, which enables us to study the interactions between the subnetworks and how hubs play their role in information flow across different subnetworks. We further demonstrated that the assignments of interacting/individual nodes using our approach correspond with the posterior probability derived independently from our multimodal random walker based approach. Keywords: Clique, Overlapping Brain Subnetwork Extraction, Multisource Fusion, Functional Connectivity, Hypergraph "
1801_01822_basal_ganglia1.tex," Parkinsonism leads to various electrophysiological changes in the basal ganglia-thal \-amo \-cort \-ical system (BGTCS), often including elevated discharge rates of the subthalamic nucleus (STN) and the output nuclei, and reduced activity of the globus pallidus external segment (GPe). These rate changes have been explained qualitatively in terms of the direct/indirect pathway model, involving projections of distinct striatal populations to the output nuclei and GPe. Although these populations partly overlap, evidence suggests dopamine depletion differentially affects cortico-striato-pallidal connection strengths to the two pallidal segments. Dopamine loss may also decrease the striatal signal-to-noise ratio, reducing both corticostriatal coupling and striatal firing thresholds. Additionally, nigrostriatal degeneration may cause secondary changes including weakened lateral inhibition in the GPe, and mesocortical dopamine loss may decrease intracortical excitation and especially inhibition. Here a mean-field model of the BGTCS is presented with structure and parameter estimates closely based on physiology and anatomy. Changes in model rates due to the possible effects of dopamine loss listed above are compared with experiment. Our results suggest that a stronger indirect pathway, possibly combined with a weakened direct pathway, is compatible with empirical evidence. However, altered corticostriatal connection strengths are probably not solely responsible for substantially increased STN activity often found. A lower STN firing threshold, weaker intracortical inhibition, and stronger striato-GPe inhibition help explain the relatively large increase in STN rate. Reduced GPe-GPe inhibition and a lower GPe firing threshold can account for the comparatively small decrease in GPe rate frequently observed. Changes in cortex, GPe, and STN help normalize the cortical rate, also in accord with experiments. The model integrates the basal ganglia into a unified framework along with an existing thalamocortical model that already accounts for a wide range of electrophysiological phenomena. A companion paper discusses the dynamics and oscillations of this combined system. "
1801_06223_ms_pfvv.tex,"\abstract{ A range of astronomical data indicates that ancient  supernovae created the galactic environment of the Sun and sculpted  the physical properties of the interstellar medium near the  heliosphere. In this paper we review the characteristics of the  local interstellar medium that have been affected by supernovae.  The kinematics, magnetic field, elemental abundances, and  configuration of the nearest interstellar material support the view  that the Sun is at the edge of the Loop I superbubble, which has  merged into the low density Local Bubble. The energy source for the  higher temperature X-ray emitting plasma pervading the Local Bubble  is uncertain. Winds from massive stars and nearby supernovae,  perhaps from the Sco-Cen Association, may have contributed  radioisotopes found in the geologic record and galactic cosmic ray  population. Nested supernova shells in the Orion and Sco-Cen  regions suggest spatially distinct sites of episodic star formation.  The heliosphere properties vary with the pressure of the surrounding  interstellar cloud. A nearby supernova would modify this pressure   equilibrium and thereby severely disrupt the heliosphere as well  as the local interstellar medium. }"
1801_10206.tex," For a stationary spacetime metric, black holes are spatial regions which disturbances may not propagate out of. In our previous work an existence and regularity theorem was proven for black holes in two space dimensions, in the case where the boundary of the ergoregion is a simple closed curve surrounding a singularity. In this paper we study the case of an annular ergoregion, whose boundary has two components. "
1801_08275_actamat_drx_04a.tex," %% Text of abstract Dynamic recrystallization (DRX) is often observed in conjunction with adiabatic shear banding (ASB) in polycrystalline materials. The recrystallized nanograins in the shear band have few dislocations compared to the material outside of the shear band. In this paper, we reformulate the recently-developed Langer-Bouchbinder-Lookman (LBL) continuum theory of polycrystalline plasticity and include the creation of grain boundaries. While the shear-banding instability emerges because thermal heating is faster than heat dissipation, recrystallization is interpreted as an entropic effect arising from the competition between dislocation creation and grain boundary formation. We show that our theory closely matches recent results in sheared ultrafine-grained titanium. The theory thus provides a thermodynamically consistent way to systematically describe the formation of shear bands and recrystallized grains therein. "
1801_09740_manuscript.tex," 	Reliable estimates of indirect economic losses arising from natural disasters are currently out of scientific reach. To address this problem, we propose a novel approach that combines a probabilistic physical damage catastrophe model with a new generation of macroeconomic agent-based models (ABMs). The ABM moves beyond the state of the art by exploiting large data sets from detailed national accounts, census data, and business information, etc., to simulate interactions of millions of agents representing each natural person or legal entity in a national economy. The catastrophe model introduces a copula approach to assess flood losses, considering spatial dependencies of the flood hazard. These loss estimates are used in a damage scenario generator that provides input for the ABM, which then estimates indirect economic losses due to the event. For the first time, we are able to link environmental and economic processes in a computer simulation at this level of detail. We show that moderate disasters induce comparably small but positive short- to medium-term, and negative long-term economic impacts. Large-scale events, however, trigger a pronounced negative economic response immediately after the event and in the long term, while exhibiting a temporary short- to medium-term economic boost. We identify winners and losers in different economic sectors, including the fiscal consequences for the government. We quantify the critical disaster size beyond which the resilience of an economy to rebuild reaches its limits. Our results might be relevant for the management of the consequences of systemic events due to climate change and other disasters. 	"
1801_05127_abstract.tex," Distributed graph algorithms that separately optimize for either the number of rounds used or the total number of messages sent have been studied extensively. However, algorithms simultaneously efficient with respect to both measures have been elusive for a long time. For example, only very recently was it shown that for Minimum Spanning Tree (MST), an optimal message and round complexity is achievable (up to polylog terms) by a single algorithm in the  model of communication. In this paper we provide algorithms that are simultaneously round-optimal with near-linear message complexities for a number of well-studied distributed optimization problems. Our algorithmic centerpiece is such a distributed algorithm that solves what we dub Part-Wise Aggregation : computing simple functions over each part of a graph partition. From this algorithm we derive round-optimal algorithms for MST, Approximate Min-Cut and Approximate Single Source Shortest Paths (SSSP), all with $O(m)$ message complexities. On general graphs all of our algorithms achieve a worst-case optimal $O(D+\sqrt n)$ round complexities and $O(m)$ message complexities.  Furthermore, our algorithms require even fewer rounds on many widely-studied classes of graphs, namely planar, genus-bounded, treewidth-bounded and pathwidth-bounded graphs. For these graphs our algorithms require an optimal $O(D)$ rounds and $O(m)$ messages. Our results are the first instance of distributed algorithms with $O(m)$ message complexities for Approximate Min-Cut and Approximate SSSP. Moreover, our algorithms are the first algorithms for any of these problems that beat the general graph round lower bound of $\Omega(D + n)$ on graph families of interest and simultaneously achieve an $O(m)$ message complexity. %Given the ubiquity of Part-Wise Aggregation, we believe our algorithms for this basic algorithmic primitive will find further uses for simultaneously round- and message-efficient algorithms. I'm not sure that this is all that cogent at this point since PA is something we introduce. "
1801_01819.tex," In this paper, we study twisted arithmetic divisors on the modular curve $\mathcal X_0(N)$ with $N$ square-free. For each pair $(\Delta, r)$ where $\Delta >0$ and $\Delta \equiv r^2 \mod 4N$ , we constructed a twisted arithmetic theta function $\phi_{\Delta, r}(\tau)$ which is a generating function of arithmetic twisted Heegner divisors. We prove the modularity of $\phi_{\Delta, r}(\tau)$ , along the way, we also identify the arithmetic pairing  $\phi_{\omega_N \rangle$  with special value of some Eisenstein series, where $\omega_N$ is a normalized metric Hodge line bundle. "
1801_00718_main.tex," 		In this work, methods to detect one or several change points in multivariate time series are reviewed. 		They include retrospective (off-line) procedure such as maximum likelihood estimation, regression, kernel methods, etc. 		In this large area of research, applications are numerous and diverse; many different models and operational constraints (on precision, complexity,...) exist. 		A formal framework for change point detection is introduced to give sens to this significant body of work. 		Precisely, all methods are described as a collection of three elements: a cost function, a search method and a constraint on the number of changes to detect. 		For a given method, we detail the assumed signal model, the associated algorithm, theoretical guarantees (if any) and the application domain. 		This approach is intended to facilitate prototyping of change point detection methods: for a given segmentation task, one can appropriately choose among the described elements to design an algorithm. 	"
1801_07365_egpaper_for_review.tex," Many state-of-the-art computer vision algorithms use large scale convolutional neural networks (CNNs) as basic building blocks. These CNNs are known for their huge number of parameters, high redundancy in weights, and tremendous computing resource consumptions. This paper presents a learning algorithm to simplify and speed up these CNNs. Specifically, we introduce a ``try-and-learn"" algorithm to train pruning agents that remove unnecessary CNN filters in a data-driven way. With the help of a novel reward function, our agents removes a significant number of filters in CNNs while maintaining performance at a desired level. Moreover, this method provides an easy control of the tradeoff between network performance and its scale. Performance of our algorithm is validated with comprehensive pruning experiments on several popular CNNs for visual recognition and semantic segmentation tasks. "
1801_00497_Bayes.tex," Magnetic tunnel junctions (MTJ's) with low barrier magnets have been used to implement random number generators (RNG's) and it has recently been shown that such an MTJ connected to the drain of a conventional transistor provides a three-terminal tunable RNG or a  $p$ -bit. In this letter we show how this $p$ -bit can be used to build a $p$ -circuit that emulates a Bayesian network (BN), such that the correlations in real world variables can be obtained from electrical measurements on the corresponding circuit nodes. The $p$ -circuit design proceeds in two steps: the BN is first translated into a behavioral model, called Probabilistic Spin Logic (PSL), defined by dimensionless biasing (h) and interconnection (J) coefficients, which are then translated into electronic circuit elements. As a benchmark example, we mimic a family tree of three generations and show that the genetic relatedness calculated from a SPICE-compatible circuit simulator matches well-known results. "
1801_09391_mmWave_PPP_two.tex," Millimeter wave offers a sensible solution to the capacity crunch faced by 5G wireless communications. This paper comprehensively studies physical layer security in a multi-input single-output (MISO) millimeter wave system where multiple single-antenna eavesdroppers are randomly located. Concerning the specific propagation characteristics of millimeter wave, we investigate two secure transmission schemes, namely maximum ratio transmitting (MRT) beamforming and artificial noise (AN) beamforming. Specifically, we first derive closed-form expressions of the connection probability for both schemes. We then analyze the secrecy outage probability (SOP) in both non-colluding eavesdroppers and colluding eavesdroppers scenarios. Also, we maximize the secrecy throughput under a SOP constraint, and obtain optimal transmission parameters, especially the power allocation between AN and the information signal for AN beamforming. Numerical results are provided to verify our theoretical analysis. We observe that the density of eavesdroppers, the spatially resolvable paths of the destination and eavesdroppers all contribute to the secrecy performance and the parameter design of millimeter wave systems. "
1801_08038_mech_annealing_high_temp_arxiv.tex," Non-equilibrium molecular dynamics simulations are performed to investigate the dynamic behavior of three-dimensional binary glasses prepared via an instantaneous quench across the glass transition. We found that with increasing strain amplitude up to a critical value, the potential energy approaches lower minima in steady state, whereas the amplitude of shear stress oscillations becomes larger. Below the yielding transition, the storage modulus dominates the mechanical response, and the gradual decay of the potential energy over consecutive cycles is accompanied by reduction in size of transient clusters of atoms with large nonaffine displacements. In contrast, above the yield strain, the loss modulus increases and the system settles at a higher level of potential energy due to formation of a system-spanning shear band after a number of transient cycles.  \vskip 0.5in Keywords: glasses, deformation, temperature, strain amplitude, molecular dynamics simulations "
1801_05192_lcws-SUSYProd-proc.tex," \noindent For the search for electroweak particles in the Minimal Supersymmetric Standard Model (MSSM) as well as for future precision analyses of these particles an accurate knowledge of their production and decay properties is mandatory. We review the evaluation of the cross sections for the chargino, neutralino and scalar lepton production  at $e^+e^-$ colliders in the MSSM with complex parameters (cMSSM). The evaluation is based on a full one-loop calculation of the various production mechanisms, including soft and hard photon radiation.  The dependence of the chargino/neutralino/slepton cross sections on the relevant  cMSSM parameters is analyzed numerically. We find sizable contributions to many production cross sections. They amount roughly $\pm 15\,\%$ of the tree-level results, but can go up to $\pm 40\,\%$ or higher in extreme cases. Also the complex phase dependence of the one-loop corrections was found non-negligible. The full one-loop contributions are thus crucial for physics analyses at a future linear $e^+e^-$ collider such as the ILC or CLIC. "
1801_04363_KT_MS_Main_ver01.tex," We propose a simple and effective method for designing approximation formulas for weighted analytic functions. We consider spaces of such functions according to weight functions expressing the decay properties of the functions. Then, we adopt the minimum worst error of the  $n$ -point approximation formulas in each space for characterizing the optimal sampling points for the approximation. In order to obtain approximately optimal sampling points, we consider minimization of a discrete energy related to the minimum worst error. Consequently, we obtain an approximation formula and its theoretical error estimate in each space. In addition, from some numerical experiments, we observe that the formula generated by the proposed method outperforms the corresponding formula derived with sinc approximation, which is near optimal in each space. "
1801_06890_Aging_Resub_arXiv.tex," This paper derives and discusses the configuration-space Langevin equation describing a physically aging R-simple system and the corresponding Smoluchowski equation. Externally controlled thermodynamic variables like temperature, density, pressure enter the description via the single parameter $\Teq/T$ in which $T$ is the bath temperature and $\Teq$ is the ``systemic'' temperature defined at any time $t$ as the thermodynamic equilibrium temperature of the state point with density $\rho(t)$ and potential energy $U(t)$ . In equilibrium $\Teq\cong T$ with fluctuations that vanish in the thermodynamic limit. In contrast to Tool's fictive temperature and other effective temperatures in glass science, the systemic temperature is defined for any configuration with a well-defined density, even if it is not in any sense close to equilibrium. Density and systemic temperature define an aging phase diagram in which the aging system traces out a curve. Predictions are discussed for aging following various density-temperature and pressure-temperature jumps from one equilibrium state to another, as well as for a few other scenarios. The proposed theory implies that R-simple glass-forming liquids are characterized by a dynamic Prigogine-Defay ratio of unity. "
1801_00136_CoSnS_Arc.tex," Very recently, the half-metallic compound Co $_3$ Sn $_2$ S $_2$ was predicted to be a magnetic WSM with Weyl points only 60 meV above the Fermi level ( $E_F$ ). Owing to the low charge carrier density and large Berry curvature induced, Co $_3$ Sn $_2$ S $_2$ possesses both a large anomalous Hall conductivity (AHC) and a large anomalous Hall angle (AHA), which provide strong evidence for the existence of Weyl points in Co $_3$ Sn $_2$ S $_2$ . In this work, we theoretically studied the surface topological feature of Co $_3$ Sn $_2$ S $_2$ and its counterpart Co $_3$ Sn $_2$ Se $_2$ . By cleaving the sample at the weak Sn--S/Se bonds, one can achieve two different surfaces terminated with Sn and S/Se atoms, respectively. The resulting Fermi arc related states can range from the energy of the Weyl points to $E_F$ --0.1 eV in the Sn-terminated surface. Therefore, it should be possible to observe the Fermi arcs in angle-resolved photoemission spectroscopy (ARPES) measurements. Furthermore, in order to simulate quasiparticle interference (QPI) in scanning tunneling microscopy (STM) measurements, we also calculated the joint density of states (JDOS), which revealed that the QPI patterns arising from Fermi arc related scatterings are clearly visible for both terminals. This work would be helpful for a comprehensive understanding of the topological properties of these two magnetic WSMs and further ARPES and STM measurements. "
1801_09446_Main.tex," The recent GW170817 measurement favors the simplest dark energy models, such as a single scalar field. Quintessence models can be classified in two classes, freezing and thawing, depending on whether the equation of state decreases towards $-1$ or departs from it. In this paper we put observational constraints on the parameters governing the equations of state of tracking freezing, scaling freezing and thawing models using updated data, from the Planck 2015 release, joint light-curve analysis and baryonic acoustic oscillations. Because of the current tensions on the value of the Hubble parameter $H_0$ , unlike previous authors, we let this parameter vary, which modifies significantly the results. Finally, we also derive constraints on neutrino masses in each of these scenarios. "
1801_08284_WWW2018.tex," 	Online news recommender systems aim to address the information explosion of news and make personalized recommendation for users. 	In general, news language is highly condensed, full of knowledge entities and common sense. 	However, existing methods are unaware of such external knowledge and cannot fully discover latent knowledge-level connections among news. 	The recommended results for a user are consequently limited to simple patterns and cannot be extended reasonably. 	%Moreover, news recommendation also faces the challenges of high time-sensitivity of news and dynamic diversity of users' interests. 	To solve the above problem, in this paper, we propose a deep knowledge-aware network (DKN) that incorporates knowledge graph representation into news recommendation. 	DKN is a content-based deep recommendation framework for click-through rate prediction. 	The key component of DKN is a multi-channel and word-entity-aligned knowledge-aware convolutional neural network (KCNN) that fuses semantic-level and knowledge-level representations of news. 	KCNN treats words and entities as multiple channels, and explicitly keeps their alignment relationship during convolution. 	In addition, to address users' diverse interests, we also design an attention module in DKN to dynamically aggregate a user's history with respect to current candidate news. 	Through extensive experiments on a real online news platform, we demonstrate that DKN achieves substantial gains over state-of-the-art deep recommendation models. 	%We also validate the efficacy of the usage of knowledge in DKN. "
1801_10387_comp-ah.tex," 	We investigate the relative computability of exchangeable binary relational data when presented in terms of the distribution of an invariant measure on graphs, or as 	a graphon in either $L^1$ or the cut distance. 	We establish basic computable equivalences, and show that $L^1$ representations contain fundamentally more computable information than the other representations, but that $0'$ suffices to move between computable such representations. We show that $0'$ is necessary in general, but that in the case of random-free graphons, no oracle is necessary. We also provide an example of an $L^1$ -computable random-free graphon that is not weakly isomorphic to any graphon with an a.e. \ continuous version. "
1801_09816_version09.tex, We introduce a new and robust method for characterizing spatially and temporally heterogeneous behavior within a system based on the evolution of dynamic fuctuations once averaged over different space lengths and time scales. We apply it to investigate the dynamics in two canonical systems as the glass transition is approached: a simulated Lennard-Jones glass-former and a real dense colloidal suspensions. We find that in both cases the onset of glassines is marked by spatially localized dynamic fluctuations originating in regions of correlated mobile particles. By removing the trivial system size dependence of the fluctuations we show that such regions contain tens to hundreds of particles for time scales corresponding to maximally non-Gaussian dynamics.
1801_09703_MultiPRL.tex,"  We study the fragmentation of a jet propagating in a dense  quark-gluon plasma.  %  Using a leading, double-logarithmic approximation in perturbative  QCD, we compute for the first time the effects of the medium on the  vacuum-like emissions.  %  We show that, due to the scatterings off the plasma, the in-medium  parton showers differ from the vacuum ones in two crucial aspects:  their phase-space is reduced and the first emission outside the  medium can violate angular ordering.  %  We compute the jet fragmentation function and find results in  qualitative agreement with measurements at the LHC. "
1801_08093_main.tex," Learning locomotion skills is a challenging problem. To generate realistic and smooth locomotion, existing methods use motion capture, finite state machines or morphology-specific knowledge to guide the motion generation algorithms. Deep reinforcement learning (DRL) is a promising approach for the automatic creation of locomotion control. Indeed, a standard benchmark for DRL is to automatically create a running controller for a biped character from a simple reward function duan2016benchmarking . Although several different DRL algorithms can successfully create a running controller, the resulting motions usually look nothing like a real runner. This paper takes a minimalist learning approach to the locomotion problem, without the use of motion examples, finite state machines, or morphology-specific knowledge. We introduce two modifications to the DRL approach that, when used together, produce locomotion behaviors that are symmetric, low-energy, and much closer to that of a real person. First, we introduce a new term to the loss function (not the reward function) that encourages symmetric actions. Second, we introduce a new curriculum learning method that provides modulated physical assistance to help the character with left/right balance and forward movement. The algorithm automatically computes appropriate assistance to the character and gradually relaxes this assistance, so that eventually the character learns to move entirely without help. Because our method does not make use of motion capture data, it can be applied to a variety of character morphologies. We demonstrate locomotion controllers for the lower half of a biped, a full humanoid, a quadruped, and a hexapod. Our results show that learned policies are able to produce symmetric, low-energy gaits. In addition, speed-appropriate gait patterns emerge without any guidance from motion examples or contact planning. "
1801_01127_HF_v4.tex," Self-learning Monte Carlo (SLMC) method is a general algorithm to speedup MC simulations. Its efficiency has been demonstrated in various systems by introducing an effective model to propose global moves in the configuration space. In this paper, we show that deep neural networks can be naturally incorporated into SLMC, and without any prior knowledge, can accurately learn the original model accurately and efficiently. Demonstrated in quantum impurity models, we reduce the complexity for a local update from $ O(\beta^2) $ in Hirsch-Fye algorithm to $ O(\beta \log \beta) $ , which is a significant speedup especially for systems at low temperatures. "
1801_08781_2017FTGal.tex," Models of the very early universe, including inflationary models, are argued to produce varying universe domains with different values of fundamental constants and cosmic parameters. Using the cosmological hydrodynamical simulation code from the \eagle collaboration, we investigate the effect of the cosmological constant on the formation of galaxies and stars. We simulate universes with values of the cosmological constant ranging from $\Lambda = 0$ to $\Lambda_0 \times 300$ , where $\Lambda_0$ is the value of the cosmological constant in our Universe. Because the global star formation rate in our Universe peaks at $t = 3.5$ Gyr, before the onset of accelerating expansion, increases in $\Lambda$ of even an order of magnitude have only a small effect on the star formation history and efficiency of the universe. We use our simulations to predict the observed value of the cosmological constant, given a measure of the multiverse. Whether the cosmological constant is successfully predicted depends crucially on the measure. The impact of the cosmological constant on the formation of structure in the universe is not a sharp enough function of $\Lambda$ to explain its observed value alone. "
1801_09108_DNNCRF_Arxiv.tex," %  Supervised discriminative learning has been one of the leading methodologies for advancing research in visual recognition. One of the major difficulties in taking such an approach is collecting sufficient trustworthy labeled data for training. %Multimedia data like text, images and videos are being produced and shared at an unprecedented and accelerating pace in recent decades.  We propose a novel method for predicting image labels by fusing image content descriptors with the social media context of each image. An image uploaded to a social media site such as Flickr often has meaningful, associated information, such as comments and other images the user has uploaded, that is complementary to pixel content and helpful in predicting labels. Prediction challenges such as ImageNet~ imagenet_cvpr09 and MSCOCO~ LinMBHPRDZ:ECCV14 use only pixels, while other methods make predictions purely from social media context McAuleyECCV12 . Our method is based on a novel fully connected Conditional Random Field (CRF) framework, where each node is an image, and consists of two deep Convolutional Neural Networks (CNN) and one Recurrent Neural Network (RNN) that model both textual and visual node/image information. The edge weights of the CRF graph represent textual similarity and link-based metadata such as user sets and image groups. We model the CRF as an RNN for both learning and inference, and incorporate the weighted ranking loss and cross entropy loss into the CRF parameter optimization to handle the training data imbalance issue. Our proposed approach is evaluated on the MIR-9K dataset and experimentally outperforms current state-of-the-art approaches. "
1801_08912_ACC_2018v2.tex," We study the problem of collaboratively estimating the state of an LTI system monitored by a network of sensors, subject to the following important practical considerations: (i) certain sensors might be arbitrarily compromised by an adversary and (ii) the underlying communication graph governing the flow of information across sensors might be time-varying. We first analyze a scenario involving intermittent communication losses that preserve certain information flow patterns over bounded intervals of time. By equipping the sensors with adequate memory, we show that one can obtain a fully distributed, provably correct state estimation algorithm that accounts for arbitrary adversarial behavior, provided certain conditions are met by the network topology. We then argue that our approach can handle bounded communication delays as well. Next, we explore a case where each communication link stochastically drops packets based on an analog erasure channel model. For this setup, we propose state estimate update and information exchange rules, along with conditions on the network topology and packet drop probabilities, that guarantee mean-square stability despite arbitrary adversarial attacks. "
1801_04430_main.tex," We explore nonlinear transitions of polariton wavepackets, first, to a soliton and then to a standing wave polariton condensate in a multi-mode microwire system. At low polariton density we observe ballistic propagation of the multi-mode polariton wavepackets arising from the interference between different transverse modes. With increasing polariton density, the wavepackets transform into single mode bright solitons due to effects of both inter-modal and intra-modal polariton-polariton scattering. Further increase of the excitation density increases thermalisation speed leading to relaxation of the polariton density distribution in momentum space with the resultant formation of a non-equilibrium condensate manifested by a standing wave pattern across the whole sample. "
1801_07772_ijcnlp2017.tex," While neural machine translation (NMT) models  provide improved translation quality in an elegant, end-to-end  framework, it is less clear what they  learn about language.  Recent work has started evaluating the quality of vector representations learned by NMT models on morphological and syntactic tasks.  In this paper, we investigate the representations learned at different layers of NMT encoders. We train NMT systems on parallel data and use the trained %  models to extract features for training a classifier on two tasks: part-of-speech and semantic tagging. We then measure the performance of the classifier as a proxy to the quality of the original NMT model for the given task. Our quantitative analysis yields interesting insights regarding representation learning in NMT models. For instance, we find that higher layers are better at learning semantics while lower layers  tend to be better for part-of-speech tagging. We also observe little effect of the target language on source-side representations, especially with higher quality NMT models. Our code is available at \url{https://github.com/boknilev/nmt-repr-analysis.} "
1801_04860_31865_corr.tex,"\abstract{We investigate the star formation history and the dust attenuation in the galaxy merger Mrk848. Thanks to the multiwavelength photometry from the ultraviolet (UV) to the infrared (IR), and MaNGA's integral field spectroscopy, we are able to study this merger in a detailed way. We divide the whole merger into the core and tail regions, and fit both the optical spectrum and the multi-band spectral energy distribution (SED) to models to obtain the star formation properties for each region respectively. We find that the color excess of stars in the galaxy $E(B-V)_s^{\rm SED}$ measured with the multi-band SED fitting is consistent with that estimated both from the infrared excess (the ratio of IR to UV flux) and from the slope of the UV continuum. Furthermore, the reliability of the $E(B-V)_s^{\rm SED}$ is examined with a set of mock SEDs, showing that the dust attenuation of the stars can be well constrained by the UV-to-IR broadband SED fitting. The dust attenuation obtained from optical continuum $E(B-V)_s^{\rm spec}$ is only about half of $E(B-V)_s^{\rm spec$ to the $E(B-V)_g$ obtained from the Balmer decrement is consistent with the local value (around 0.5). The difference between the results from the UV-to-IR data and the optical data is consistent with the picture that younger stellar populations are attenuated by an extra dust component from the birth clouds compared to older stellar populations which are only attenuated by the diffuse dust.  Both with the UV-to-IR SED fitting and the spectral fitting, we find that there is a starburst younger than 100~Myr in one of the two core regions, consistent with the scenario that the interaction-induced gas inflow can enhance the star formation in the center of galaxies.} "
1801_03544_novak_proc_bgl2017.tex,"\abstract{Bose-Einstein correlations of identical hadrons reveal information about hadron creation from the strongly interacting matter formed in ultrarelativistic heavy ion collisions. The measurement of three-particle correlations may in particular shed light on hadron creation mechanisms beyond thermal/chaotic emission. In this paper we show the status of PHENIX measurements of three pion correlations as a function of momentum differences within the triplets. We analyze the shape of the correlation functions through the assumption of L\'evy sources and a proper treatment of the Coulomb interaction within the triplets. We measure the three-particle correlation strength ($\lambda_3$), which, together with the two-particle correlation strength $\lambda_2$, encodes information about hadron creation mechanisms. From a consistent analysis of two- and three-particle correlation strength we establish a new experimental measure of thermalization and coherence in the source. }"
1801_05564_ICITC-JOH.tex," We analyze socialbots active tweeting in relation to Juan Orlando Hern \em Botometer the recently re-elected president of Honduras. We find a clear bimodal separation between humans and bots, using  and its classifiers. Around one hundred separate communities of socialbots are identified and visualized, detected through the analysis of temporally coordinated retweets.  social network analysis, socialbots, elections, Spanish, Honduras "
1801_01156_elsarticle-template.tex," Most of the non-linear transceivers, which are based on Tomlinson Harashima (TH) precoding and have been proposed in the literature for two-way relay networks, assume perfect channel state information (CSI). In this paper, we propose a novel and robust TH precoding scheme for two-way relay networks with multiple antennas at the transceiver and the relay nodes. We assume imperfect CSI and that the channel uncertainty is bounded by a spherical region. Furthermore, we consider the sum of the mean square error as the objective function, under a limited power constraint for transceiver and relay nodes. Simulations are provided to evaluate the performance and to validate the efficiency of the proposed scheme. "
1801_04949_main.tex," We present a study of the M dwarf exoplanetary systems forthcoming from NASA's {\it TESS} mission. While the mission's footprint is too complex to be characterized by a single detection completeness, we extract an ensemble completeness function that recovers the M dwarf planet detections from previous work. We employ this completeness function, together with a dual-population planet occurrence model that includes compact multiple planetary systems, to infer anew the planet yield. We predict both the number of M dwarf planets likely from {\it TESS} and their system architectures. We report four main findings: first, that {\it TESS} will likely detect more planets orbiting M dwarfs that previously predicted. Around stars with spectral types between M1V--M4V, we predict {\it TESS} will find 990 $\pm$ 350 planets orbiting 715 $\pm$ 255 stars, a 1.5-fold increase over previous predictions. Secondly, {\it TESS} will find two or more transiting planets around 20 \it Kepler of these host stars, a number similar to the multiplicity yield of NASA's  mission. Thirdly, {\it TESS} light curves in which one or more planets are detected will often contain transits of additional planets below the detection threshold of {\it TESS} . Among a typical set of 200 {\it TESS} hosts to one or more detected planets, 116 $\pm$ 28 transiting planets will be missed. Transit follow-up efforts with the photometric sensitivity to detect an Earth or larger around a mid-M dwarf, even with very modest period completeness, will readily result in additional planet discoveries. And fourth, the strong preference of {\it TESS} for systems of compact multiples indicates that {\it TESS} planets will be dynamically cooler on average than {\it Kepler} planets, with 90 \it TESS of  planets residing in orbits with $e<0.15$ . "
1801_04638.tex," For a variety of finite groups $\bH$ , let $\bHbar$ denote the variety of finite semigroups all of whose subgroups lie in $\bH$ . We give a characterization of the subsets of a finite semigroup that are pointlike with respect to $\bHbar$ . Our characterization is effective whenever $\bH$ has a decidable membership problem. In particular, the separation problem for $\bHbar$ -languages is decidable for any decidable variety of finite groups $\bH$ . This generalizes Henckell's theorem on decidability of aperiodic pointlikes. "
1801_01905_draft_v13.tex," The detection of significant \gm-ray emission from radio-loud narrow line Seyfert 1 (NLSy1s) galaxies enables us to study jets in environments different than those in blazars. However, due to the small number of known \gm-ray emitting NLSy1 ( \gm-NLSy1) galaxies, a comprehensive study could not be performed. Here we report the first detection of significant \gm-ray emission from four active galactic nuclei (AGN), recently classified as NLSy1 from their Sloan Digital Sky Survey (SDSS) optical spectrum. Three flat spectrum radio quasars (FSRQs) present in the third Large Area Telescope AGN catalog (3LAC) are also found as \gm-NLSy1 galaxies. Comparing the \gm-ray properties of these objects with 3LAC blazars reveals their spectral shapes to be similar to FSRQs, however, with low 46-47$  \beta$ emission line parameters, we find that on average \gm-NLSy1 have smaller black hole masses than FSRQs at similar redshifts. In the low-resolution SDSS image of one of the \gm-NLSy1 source, we find the evidence of an extended structure. We conclude by noting that overall many observational properties of \gm-NLSy1 sources are similar to FSRQs and therefore, these objects could be their low black hole mass counterparts, as predicted in the literature. "
1801_10274_6th_draft.tex," It was recently found that the Lee-Huang-Yang (LHY) correction to the mean-field Hamiltonian suppresses the collapse and creates stable localized modes (two-component ``quantum droplets"", QDs) in two and three dimensions. We construct two-dimensional \ self-trapped modes in the form of QDs with vorticity $S$ embedded into each component. The QDs feature a flat-top shape, which expands with the increase of $S$ and norm $N$ . An essential finding, produced by a systematic numerical analysis and analytical estimates, is that the vortical QDs are stable (which is a critical issue for vortex solitons in nonlinear models) up to $S=5$ , for $N$ exceeding a certain threshold value. In the condensate of $^{39}$ K atoms, in which QDs with $S=0$ and a quasi-2D shape were created recently, the vortical droplets may have radial size $\lesssim 30$  $\mu $ m, with the number of atoms in the range of $10^{4}-10^{5}$ . It is worthy to note that hidden-vorticity states in QDs with topological charges $% S_{+}=-S_{-}=1$ in its components, which are prone to strong instability in other settings, have their stability region too, although it may be located beyond applicability limits of the underlying model. Dynamics of elliptically deformed QDs, which form rotating elongated patterns or ones with strong oscillations of the eccentricity, as well as collisions of QDs, are also addressed. "
1801_03686_MVerma_2017.tex, % % context heading (optional)
1801_07116_v5.tex," Laying a basis for molecularly specific theory for the mobilities of ions in solutions of practical interest, we report a broad survey of velocity autocorrelation functions (VACFs) of Li $^+$ and PF $_6{}^-$ ions in water, ethylene carbonate, propylene carbonate, and acetonitrile solutions. We extract the memory function, $\gamma(t)$ , which characterizes the random forces governing the mobilities of ions. We provide comparisons, controlling for electrolyte concentration and ion-pairing, for van~der~Waals attractive interactions and solvent molecular characteristics. For the heavier ion (PF $_6{}^-$ ), velocity relaxations are all similar: negative tail relaxations for the VACF and a clear second relaxation for $\gamma\left(t\right)$ , observed previously also for other molecular ions and with n -pentanol as solvent. For the light Li $^+$ ion, short time-scale oscillatory behavior masks simple, longer time-scale relaxation of $\gamma\left(t\right)$ . But the corresponding analysis of the solventberg  Li $^+H_2O\right)_4$ does conform to the standard picture set by all the PF $_6{}^-$ results.  "
1801_02937_IncrementalValidityIndex.tex," Cluster analysis is used to explore structure in unlabeled data sets in a wide range of applications. An important part of cluster analysis is validating the quality of computationally obtained clusters. A large number of different internal indices have been developed for validation in the offline setting. However, this concept has not been extended to the online setting. A key challenge is to find an efficient incremental formulation of an index that can capture both cohesion and separation of the clusters over potentially infinite data streams. In this paper, we develop two online versions (with and without forgetting factors) of the Xie-Beni and Davies-Bouldin internal validity indices, and analyze their characteristics, using two streaming clustering algorithms (sk-means and online ellipsoidal clustering), and illustrate their use in monitoring evolving clusters in streaming data. We also show that incremental cluster validity indices are capable of sending a distress signal to online monitors when evolving clusters go awry. Our numerical examples indicate that the incremental Xie-Beni index with forgetting factor is superior to the other three indices tested. "
1801_02588_FRAG_GUAL.tex," The last decade has seen the detection of fast moving stars in the Galactic halo, the so-called hypervelocity stars (HVSs). While the bulk of this population is likely the result of a close encounter between a stellar binary and the supermassive black hole (MBH) in the Galactic Centre (GC), other mechanims may contribute fast stars to the sample. Few observed HVSs show apparent ages which are shorter than the flight time from the GC, thereby making the binary disruption scenario unlikely. These stars may be the result of the breakup of a stellar triple in the GC which led to the ejection of a hypervelocity binary (HVB). If such binary evolves into a blue straggler star due to internal processes after ejection, a rejuvenation is possible that make the star appear younger once detected in the halo. A triple disruption may also be responsible for the presence of HVBs, of which one candidate has now been observed. We present a numerical study of triple disruptions by the MBH in the GC and find that the most likely outcomes are the production of single HVSs and single/binary stars bound to the MBH, while the production of HVBs has a probability $\lesssim 1\%$ regardless of the initial parameters. Assuming a triple fraction of $\approx 10\%$ results in an ejection rate of $-1$ , insufficient to explain the sample of HVSs with lifetimes shorter than their flight time. We conclude that alternative mechanisms are responsible for the origin of such objects and HVBs in general. "
1801_07808_AspHurKen180118.tex," Pebbling on graphs is a two-player game which involves repeatedly moving a pebble from one vertex to another by removing another pebble from the first vertex. The pebbling number $\pi(G)$ is the least number of pebbles required so that, regardless of the initial configuration of pebbles, a pebble can reach any vertex. Graham conjectured that the pebbling number for the cartesian product, $G \Osq H$ , is bounded above by $\pi(G) \pi(H)$ . We show that $\pi(G \Osq H) \le 2\pi(G) \pi(H)$ and, more sharply, that $\pi(G \Osq H) \le (\pi(G)+|G|) \pi(H)$ . Furthermore, we provide similar results for other graph products and graph operations. "
1801_09240_abstract.tex," The growing popularity of dynamic applications such as social networks provides a promising way to detect valuable information in real time. Efficient analysis over high-speed data from dynamic applications is of great significance. Data from these dynamic applications can be easily modeled as streaming graph. In this paper, we study the subgraph (isomorphism) search over streaming graph data that obeys timing order constraints over the occurrence of edges in the stream. We propose a data structure and algorithm to efficiently answer subgraph search and introduce optimizations to greatly reduce the space cost, and propose concurrency management to improve system throughput. Extensive experiments on real network traffic data and synthetic social streaming data confirms the efficiency and effectiveness of our solution.  The growing popularity of social media provides a promising way to detect realtime events. In this paper, we adopt a graph stream model for event detection over social media. Different from existing work, we propose a LIS (longest increasing subsequence)-based edge weight to evaluate the importance of a keyword co-occurrence with regard to an event. LIS-based measure conquers the limitations of existing metrics, such as ignoring the inherent correlation and sensitive to noises. More importantly, we propose a linear time and space stream algorithm to detect event subgraphs from social media. The elegant theoretical result indicates the high scalability of our graph stream solution in Web-scale social media data. Extensive experiments on real Twitter data confirms the efficiency and effectiveness of our solution. "
1801_03486_manuscript.tex," The evolution of photospheric flow and magnetic fields before and after flares can provide important information regarding the flare triggering and back reaction processes. However, such studies on the flow field are rare due to the paucity of high-resolution observations covering the entire flaring period. Here we study the structural evolution of penumbra and shear flows associated with the 2015 June 22 M6.5 flare in NOAA AR 12371, using high-resolution imaging observation in the TiO band taken by the 1.6~m Goode Solar Telescope at Big Bear Solar Observatory, with the aid of the differential affine velocity estimator method for flow tracking. The accompanied photospheric vector magnetic field changes are also analyzed using data from the Helioseismic and Magnetic Imager. As a result, we found, for a penumbral segment in the negative field adjacent to the magnetic polarity inversion line (PIL), an enhancement of penumbral flows (up to an unusually high value of \sm2~ \kms) and extension of penumbral fibrils after the first peak of the flare hard X-ray (HXR) emission. We also found an area at the PIL, which is co-spatial with a precursor brightening kernel, exhibits a gradual increase of shear flow velocity (up to \sm0.9~ \kms) after the flare. The enhancing penumbral and shear flow regions are also accompanied by an increase of horizontal field and decrease of magnetic inclination angle(measured from the solar surface). These results are discussed in the context of the theory of back reaction of coronal restructuring on the photosphere as a result of flare energy release.  "
1801_05861_GET_Iopt.tex," The generalized linear model plays an important role in statistical analysis and the related design issues are undoubtedly challenging. The state-of-the-art works mostly apply to design criteria on the estimates of regression coefficients. It is of importance to study optimal designs for generalized linear models, especially on the prediction aspects. In this work, we propose a prediction-oriented design criterion, I-optimality, and develop an efficient sequential algorithm of constructing I-optimal designs for generalized linear models. Through establishing the General Equivalence Theorem of the I-optimality for generalized linear models, we obtain an insightful understanding for the proposed algorithm on how to sequentially choose the support points and update the weights of support points of the design. The proposed algorithm is computationally efficient with guaranteed convergence property. Numerical examples are conducted to evaluate the feasibility and computational efficiency of the proposed algorithm. "
1801_09312_article.tex," Quantum thermodynamics is emerging, both as a topic of fundamental research, but also as means to understand and potentially improve the performance of quantum devices~ vinjanampathy_quantum_2016,martinezheat,goold_role_2016,pekola2015,jezouin,schwab,banerjee,sivre,cottet . Superconducting circuit QED (Quantum Electro-Dynamics)~ wallraff presents a prominent platform for the necessary manipulation of quantum states. Thermalization of a quantum system, a subject of intense current interest~ neill_ergodic_2016,srednicki_chaos_1994,kaufman,reimann_eigenstate_2015 , can be achieved by interfacing the circuit QED subsystem with a thermal reservoir of appropriate Hilbert dimensionality. In this context, we consider an assembly consisting of a superconducting qubit~ koch_charge-insensitive_2007 capacitively coupled between two nominally identical coplanar waveguide (CPW) resonators, each terminated by a normal-metal mesoscopic resistor. We report observation of tunable photonic heat transport through the resonator-qubit-resonator assembly. With the aid of a theoretical model presented here, we are able to reproduce experimental data. Importantly, the reservoir-to-reservoir heat flux depends on the interplay between the qubit-resonator and the resonator-reservoir couplings, yielding qualitatively dissimilar results in different coupling regimes. Our Quantum Heat Valve (QHV) is a technological prerequisite for the realisation of quantum heat engines~ rossnagel_single_2016 and refrigerators, that can be obtained, \eg, by exploiting the time-domain dynamics and coherence of driven superconducting qubits~ kosloff,karimi_otto_2016 . This effort would ultimately bridge the gap between the fields of quantum information and thermodynamics of mesoscopic systems. "
1801_04801_IKPFINAL.tex," We consider the 0--1 Incremental Knapsack Problem (IKP) where the capacity grows over time periods and if an item is placed in the knapsack in a certain period, it cannot be removed afterwards. The contribution of a packed item in each time period depends on its profit as well as on a time factor which reflects the importance of the period in the objective function. The problem calls for maximizing the weighted sum of the profits over the whole time horizon. In this work, we provide approximation results for IKP and its restricted variants. In some results, we rely on Linear Programming (LP) to derive approximation bounds and show how the proposed LP--based analysis can be seen as a valid alternative to more formal proof systems. We first manage to prove the tightness of some approximation ratios of a general purpose algorithm currently available in the literature and originally applied to a time-invariant version of the problem. We also devise a Polynomial Time Approximation Scheme (PTAS) when the input value indicating the number of periods is considered as a constant. Then, we add the mild and natural assumption that each item can be packed in the first time period. For this variant, we discuss different approximation algorithms suited for any number of time periods and for the special case with two periods. "
1801_06233_manuscript.tex," \noindent We investigate the role of AGN feedback in turbulent heating of galaxy clusters. Specifically, we analyze the production of turbulence by g-modes generated by the supersonic expansion and buoyant rise of AGN-driven bubbles. Previous work which neglects magnetic fields has shown that this process is inefficient, with less than 1 \% of the injected energy ending up in turbulence. This inefficiency is primarily due to the fact that the bubbles are shredded apart by hydrodynamic instabilities before they can excite sufficiently strong g-modes. Using a plane-parallel model of the ICM and 3D ideal MHD simulations, we examine the role of a large-scale magnetic field which is able to drape around these rising bubbles, preserving them from hydrodynamic instabilities. We find that, while magnetic draping appears better able to preserve AGN-driven bubbles, the driving of g-modes and the resulting production of turbulence is still inefficient. The magnetic tension force prevents g-modes from transitioning into the nonlinear regime, suppressing turbulence in our model ICM. Our work highlights the ways in which ideal MHD is an insufficient description for the cluster feedback process, and we discuss future work such as the inclusion of anisotropic viscosity as a means of simulating high $\beta$ plasma kinetic effects. These results suggest the hypothesis that other mechanisms of heating the ICM plasma such as sound waves or cosmic rays may be responsible for observed feedback in galaxy clusters.  "
1801_10473_main.tex," %We investigate the impact of the \Lya radiation feedback on the star cluster formation induced via recombination processes associated with the star formation in the gas cloud.  %The trapping of the \Lya photons boosts the radiation pressure in the cloud, which would suppress the subsequent star formation. However, since the dust absorption reduces the photon trapping time, \Lya radiation feedback would be regulated by the metallicity in the cloud. We study the impact of \Lya radiation feedback on globular cluster (GC) formation. In this Letter, we analytically derive the relation between star formation efficiency (SFE) and metallicity in spherical clouds with the \Lya radiation feedback. Our models show that the SFE becomes small as the metallicity decreases. In metal-poor gas clouds, \Lya photons are trapped for a long time and exert strong radiation force to the gas, resulting in the suppression of star formation. We find that bound star-clusters (${\rm SFE} \gtrsim 0.5$ ) form only for the metallicity higher than $-2.5~\Zsun$ in the case with the initial cloud mass $10^{5}~\Msun$ and the radius $5~\rm pc$ . Our models successfully reproduce the lower bound of observed metallicity of GCs. Thus, we suggest that the \Lya radiation feedback can be essential in understanding the formation of GCs. %In this paper, we analytically derive the relation between the star formation efficiency (SFE) and the metallicity. We find that the SFE decreases with decreasing the metallicity since the \Lya radiation feedback efficiently works, indicating that the bound star cluster tend to be harder to form from the extremely metal-poor cloud. %Although our result retains the uncertainties of the properties of the cloud and the dust model, we show that our argument seems to be consistent with the absence of the extremely low-metal globular clusters (GCs) (${\rm [Fe/H]} \lesssim -2.5$ ) despite such low-metal stars have been observed as the single stars. %Therefore, we conclude that the \Lya radiation feedback would be the one of the essential conditions for the formation of the GCs. "
1801_02203_sample-sigconf.tex," Indian regional movie dataset is the first database of regional Indian movies, users and their ratings. It consists of movies belonging to 18 different Indian regional languages and metadata of users with varying demographics. Through this dataset, the diversity of Indian regional cinema and its huge viewership is captured. We analyze the dataset that contains roughly 10K ratings of 919 users and 2,851 movies using some supervised and unsupervised collaborative filtering techniques like Probabilistic Matrix Factorization, Matrix Completion, Blind Compressed Sensing etc. The dataset consists of metadata information of users like age, occupation, home state and known languages. It also consists of metadata of movies like genre, language, release year and cast. India has a wide base of viewers which is evident by the large number of movies released every year and the huge box-office revenue. This dataset can be used for designing recommendation systems for Indian users and regional movies, which do not, yet, exist. The dataset can be downloaded from https://goo.gl/EmTPv6{https://goo.gl/EmTPv6} . "
1801_08084_paper_gsmm_jcap3.tex,"The effect of Quantum Gravity (QG) may bring a tiny light speed variation as $v(E)=c(1-E/E_{\rm LV)$, where $E$ is the photon energy and $E_{\rm LV=3.617~\rm GeV$ determined by the GRB data.}"
1801_02294_deepmatch.tex,"     Model-based methods for recommender systems have been studied to provide more precise results.   In systems with large corpus,   the amount of calculation for learnt model to predict all user-item pairs' preferences is tremendous,   which makes the model difficult to be directly employed in recommendation candidate generation stage.   To overcome the calculation barrier, models like matrix factorization   can resort to inner product form (i.e., use the inner product of user and item's latent factors as the preference) and   index like hashing to perform efficient approximate k-nearest neighbor search.   However, other more expressive interaction forms between user and item features,   e.g., interactions through advanced deep neural networks,   are still prevented from large corpus recommendation because of the amount of calculation.   In this paper, we focus on the problem how arbitrary advanced models can be introduced to generate recommendations from large corpus.   We propose a novel tree-based method which can provide logarithmic complexity prediction   w.r.t. corpus size with more expressive deep neural networks.   The main idea of tree-based model is to predict user interests coarse-to-fine,   by traversing tree nodes top-down and making decisions whether to pick up each node to user.   Furthermore, we show that the tree structure can also be jointly learnt towards better compatible with user interests' distribution, to facilitate both training and prediction.   Experiments in two large-scale real-world datasets indicate that the proposed model significantly outperforms traditional methods.   And online A/B test results in Taobao display advertising platform prove the effectiveness of the tree-based deep model in production.    %Building recommender systems for extreme large-scale corpus is challenging in industry.   %Considering the inherent barrier caused by corpus size, widely used memory-based approaches like item-based collaborative filtering   %reduce the amount of calculation by confining candidates to a subset of the corpus through heuristic similarity rules,   %while the exploration ability of the system and the novelty of recommendation results are also limited.   %To solve the exploration problem, some model-based methods try to predict the preference score for user-item pairs.   %However, they have to estimate the preference score for each user-item pair in the entire corpus when making recommendation,   %which is and unacceptable for system with hundreds of millions of users and items.   %In this paper, we propose a novel tree-based recommendation method which can provide logarithmic complexity prediction   %w.r.t. corpus size with highly effective deep neural network.   %The main idea of tree-based model is to predict user interests from coarse to fine,   %by traversing tree nodes top down and making decisions whether to pick up each node to user.   %Furthermore, we show that the tree structure can also be learnt to refine user interests' distribution, to benefit both training and prediction.   %Experimental results in both benchmark dataset and Taobao display advertising dataset indicates that the proposed model significantly outperforms existing methods.   %And online A/B test results prove the effectiveness of the tree-based deep model in production. "
1801_05899.tex," In this paper, we introduce the concept of the complete cycle index and discuss a relation with the complete weight enumerator in coding theory. This work was motivated by Cameron's lecture note ``Polynomial aspects of codes, matroids and permutation groups.'' "
1801_02638_vaexpaper.tex,"We present a new Python library called \vaex{, to handle extremely large tabular datasets, such as astronomical catalogues like the Gaia catalogue, N-body simulations or any other regular datasets which can be structured in rows and columns. Fast computations of statistics on regular N-dimensional grids allows analysis and visualization in the order of a billion rows per second. We use streaming algorithms, memory mapped files and a zero memory copy policy to allow exploration of datasets larger than memory, e.g. out-of-core algorithms. allows arbitrary (mathematical) transformations using normal Python expressions and (a subset of) \numpy functions which are lazily evaluated and computed when needed in small chunks, which avoids wasting of RAM. Boolean expressions (which are also lazily evaluated) can be used to explore subsets of the data, which we call selections. uses a similar DataFrame API as Pandas, a very popular library, which helps migration from Pandas. Visualization is one of the key points of , and is done using binned statistics in 1d (e.g. histogram), in 2d (e.g. 2d histograms with colormapping) and 3d (using volume rendering). is split in in several packages: vaex-core for the computational part, vaex-viz for visualization mostly based on matplotlib, vaex-jupyter for visualization in the Jupyter notebook/lab based in IPyWidgets, vaex-server for the (optional) client-server communication, vaex-ui for the Qt based interface, vaex-hdf5 for hdf5 based memory mapped storage, vaex-astro for astronomy related selections, transformations and memory mapped (column based) fits storage. \Vaex is open source and available under MIT license on github, documentation and other information can be found on the main website: https://vaex.io or https://github.com/maartenbreddels/vaex}"
1801_09029_bare_conf.tex," Hybrid beamforming via large antenna arrays has shown a great potential in increasing data rate for next generation of cellular networks. It has been shown that several data streams can be delivered simultaneously in such hybrid systems. In this paper, different algorithms are proposed for designing beamforming vectors in such systems. It is assumed that the macro base station is equipped with a massive phased array incapable of adjusting the amplitude gain and phase shift of the antennas at each time slot due to hardware limitations. The non-uniform distribution of users is exploited to generate multiple static beams to serve crowded areas such as shopping malls and office buildings. The problem is formulated as a network utility maximization. First, the problem is studied when the base station has a single RF chain (single beam scenario). Semi-definite relaxation (SDR) with randomization is used to solve the problem. As a second approach, a low-complexity heuristic beam composition algorithm is proposed which performs very close to the upper-bound obtained by SDR. Next, the problem is studied for a generic number of RF chains (multi-beam scenario). An iterative algorithm called gradient projection is used to numerically obtain locally optimal beamforming vectors in the multi-beam scenario. Numerical results reveal that using optimized beamforming vectors produced by the proposed algorithms can lead to close to 5x network throughput improvement over conventional LTE networks.   "
1801_04944_mnras_guide.tex," This is a guide for preparing papers for Monthly Notices of the Royal Astronomical Society using the \verb'mnras' \LaTeX \ package. It provides instructions for using the additional features in the document class. This is not a general guide on how to use \LaTeX, and nor does it replace the journal's instructions to authors. See mnras\_template.tex for a simple template. "
1801_05457.tex," Despite the several successes of deep learning systems, there are concerns about their limitations, discussed most recently by Gary Marcus. This paper discusses Marcus's concerns and some others, together with solutions to several of these problems provided by the {\em SP theory of intelligence} and its realisation in the {\em SP computer model} . The main advantages of the SP system are: relatively small requirements for data and the ability to learn from a single experience; the ability to model both hierarchical and non-hierarchical structures; strengths in several kinds of reasoning, including `commonsense' reasoning; transparency in the representation of knowledge, and the provision of an audit trail for all processing; the likelihood that the SP system could not be fooled into bizarre or eccentric recognition of stimuli, as deep learning systems can be; the SP system provides a robust solution to the problem of `catastrophic forgetting' in deep learning systems; the SP system provides a theoretically-coherent solution to the problems of correcting over- and under-generalisations in learning, and learning correct structures despite errors in data; unlike most research on deep learning, the SP programme of research draws extensively on research on human learning, perception, and cognition; and the SP programme of research has an overarching theory, supported by evidence, something that is largely missing from research on deep learning. In general, the SP system provides a much firmer foundation than deep learning for the development of artificial general intelligence. "
0_abstract.tex," -8px  While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on the ImageNet classification task has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called ``perceptual losses""? What elements are critical for their success? To answer these questions, we introduce a new Full Reference Image Quality Assessment (FR-IQA) dataset of perceptual human judgments, orders of magnitude larger than previous datasets. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by huge margins. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations. "
