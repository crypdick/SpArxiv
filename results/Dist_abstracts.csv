old_file,abstract
1801_01968_NEC2DQN_en.tex," The research on deep reinforcement learning which estimates Q-value by deep learning has been attracted the interest of researchers recently. In deep reinforcement learning, it is important to efficiently learn the experiences that an agent has collected by exploring environment. In this research, we propose NEC2DQN that improves learning speed of a poor sample efficiency algorithm such as DQN by using good one such as NEC at the beginning of learning. We show it is able to learn faster than Double DQN or N-step DQN in the experiments of Pong. "
1801_02570_Asenjo_Moya_PoP.tex," It is well--known that when magnetic monopoles are introduced in plasma equations the propagation of electromagnetic waves is modified. This occurs because of Maxwell equations acquire a symmetrical structure due to the existence of electric and magnetic charge current densities. In this work we study the nonlinear phenomena of ponderomotive forces associated to the presence of magnetic monopoles in a plasma. We generalize the Washimi-Karpman result for the ponderomotive force on electric charges to take into account the symmetrical form of Maxwell equations in the presence of magnetic charges. It is shown that the general ponderomotive force on this plasma depends non--trivially on the magnetic monopoles, through the slowly temporal and spatial variations of the electromagnetic field amplitudes. The magnetic charges introduce corrections even if the plasma is unmagnetized. This last force induces a magnetization of the plasma, and therefore a cyclotron frequency (proportional to the square of the magnetic charge) is induced for electrons. Finally, the magnetic monopoles also experience a ponderomotive force due to the electrons. This force accelerates the magnetic charges along the direction of propagation of the electromagnetic waves. The possible consequences for experimental detection are discussed. "
1801_07833_On_a_parameter-dependent_symbolization_in_complexity_detection.tex," Symbolization is prerequisite for symbolic time series analysis which is important in nonlinear dynamic complexity analysis. Shannon entropy for a parameter-dependent four-symbols transformation is analyzed and its complexity detection is validated in our works. We analyze the symbolic entropy in complexity detection of two chaotic models, logistic and Henon maps, and use ECG and HRV derived from ECG of CHF patients, healthy young and elderly subjects to test the impacts of parameter settings on complexity analysis. The complexity-loss theory of aging and diseased heartbeats is validated and reasons that may account for the paradox of symbolic entropy in ECG are discussed. Tests results prove that original parameter settings are not universally suitable for different time series, and it is necessary to adjust the controlling parameter accordingly. "
1801_05991_suppl.tex,
1801_03551_SPCOD.tex," Detecting carried objects is one of the requirements for developing systems to reason about activities involving people and objects. We present an approach to detect carried objects from a single video frame with a novel method that incorporates features from multiple scales. Initially, a foreground mask in a video frame is segmented into multi-scale superpixels. Then the human-like regions in the segmented area are identified by matching a set of extracted features from superpixels against learned features in a codebook. A carried object probability map is generated using the complement of the matching probabilities of superpixels to human-like regions and background information. A group of superpixels with high carried object probability and strong edge support is then merged to obtain the shape of the carried object. We applied our method to two challenging datasets, and results show that our method is competitive with or better than the state-of-the-art. "
1801_02142_til1.tex," We prove the consistency of $\clubsuit$ with the negation of Galvin's property. On the other hand, we show that superclub implies Galvin's property. We also prove the consistency of $\kappa^+$ with $s_\kappa>\kappa^+$ for a supercompact cardinal $\kappa$ . "
1801_05739_Aim_09b.tex," Bell tests have become a powerful tool for quantifying security, randomness, entanglement, and many other properties, as well as for investigating fundamental physical limits. In all these cases, the specific experimental value of the Bell parameter is important as it leads to a quantitative conclusion. However, most experimental implementations aiming for high values of the Bell parameter suffer from the defect of showing signaling. This signaling can be attributed to systematic errors occurring due to weaknesses in the experimental designs. Here we point out the importance, for quantitative applications, to identify and address this problem. We present a set of experiments with polarization-entangled photons in which we point out common sources of systematic errors and demonstrate approaches to avoid them. This allows us to establish a reliable estimate for the Bell parameter. "
1801_09655_main.tex," Topological invariants allow to characterize Hamiltonians, predicting the existence of topologically protected in-gap modes. Those invariants are generically computed for infinite systems with translational symmetry, by tracing the evolution of the occupied wavefunctions under twisted boundary conditions. However, those procedures do not allow to calculate a topological invariant by evaluating the system locally, nor allow to determine it in the absence of translational symmetry. Here we show that artificial neural networks can be trained to identify the topological order by evaluating a local projection of the density matrix. We demonstrate this for two different models, a 1-D topological superconductor and a 2-D quantum anomalous Hall state, both with spatially modulated parameters. Our neural network correctly identifies the different topological domains in real space, predicting the location of in-gap states. By combining a neural network with a calculation of the electronic states that uses the Kernel Polynomial Method, we show that the local evaluation of the invariant can be carried out for systems without translational symmetry consisting of tens of thousands of atoms. Our results show that supervised learning is an efficient methodology to characterize the local topology of a system. "
1801_07920_JLTP_SI_template_LTD17.tex," Optical coupling to a lumped-element kinetic inductance detector (LEKID) via an antenna and transmission line structure enables a compact detector architecture, easily optimised for the required sensitivity and multiplexing performance of future cosmic microwave background (CMB) experiments. Coupling in this way allows multi-chroic, polarisation-sensitive pixels to be realised through planar on-chip filtering structures. However, adding the necessary dielectric layers to LEKID structures to form the microstrip-coupled architecture has the potential to increase two level system (TLS) contributions, resulting in excess detector noise. Using a lumped-element resonator enables coupling via a microstrip to the inductive section only, whilst leaving capacitive elements clear of potentially noisy dielectrics. Here we present the preliminary data acquired to demonstrate that a microstrip transmission line structure can be coupled to a LEKID architecture with minimal additional TLS contributions. This is achieved through a simple fabrication process, which allows for the dielectric to be removed from capacitive regions of the LEKID. As a result we have produced resonators with the high quality factors required for large multiplexing ratios; thus highlighting the suitability of the separated KID architecture for future observations of the CMB.  CMB, instrumentation, kinetic inductance detectors  "
1801_05762_main.tex," On an abelian scheme over a smooth curve over $\IQbar$  a symmetric relatively ample line bundle defines a fiberwise N e ron--Tate height. If the base curve is inside a projective space, we also have a height on its $\IQbar$ -points that serves as a measure of each fiber, an abelian variety.  Silverman proved an asymptotic equality between these two heights on a curve in the abelian scheme. In this paper we prove an inequality between these heights on a subvariety of any dimension of the abelian scheme. As an application we prove the Geometric Bogomolov Conjecture for the function field of a curve defined over $\IQbar$ . Using Moriwaki's height we sketch how to extend our result when the base field of the curve has characteristic $0$ . %% Given an abelian scheme over a smooth curve over $\mathbb{Q}$ , we have the N e ron--Tate height fiberwise defined by a symmetric relatively ample line bundle on this abelian scheme and a height function on the set of algebraic points of the base curve. For any irreducible subvariety $X$ of this abelian scheme, we prove that the N e ron--Tate height of any algebraic point in an explicit Zariski open subset of $X$ can be uniformly bounded from below by the height of its projection to the curve. We use this height inequality to prove the Geometric Bogomolov Conjecture over characteristic $0$ . "
1801_07877_Journal_REVISED_v6.tex," This paper investigates the design of access policies in spectrum sharing networks by exploiting the retransmission protocol of legacy primary users (PUs) to improve the spectral efficiency via opportunistic retransmissions at secondary users (SUs) and chain decoding  MichelusiCD . The optimal access policy which maximizes the SU throughput under a maximum interference constraint to the PU and its performance are found in closed form. It is shown that the optimal policy randomizes among three modes: 1) Idle: the SU remains idle over the retransmission window of the PU, to avoid interfering; 2) Interference cancellation : the SU transmits only after decoding the PU packet , to improve its own throughput via interference cancellation; 3) Always transmit : the SU transmits over the retransmission window of the PU to maximize the future potential of interference cancellation via chain decoding. This structure is exploited to design a stochastic gradient descent algorithm to facilitate learning and adaptation in settings where the model parameters are unknown or vary over time, based on ARQ feedback from the PU and CSI measurements at the SU receiver. It is shown numerically that, for a 10 \% interference constraint, the optimal access policy yields 15 \% improvement over a state-of-the-art scheme without selective SU retransmissions, and up to $2\times$ gain over a scheme using a non-adaptive access policy instead of the optimal one. "
1801_05849_cdc_draft.tex," This paper pertains to the analysis and design of decentralized estimation schemes that make use of limited communication. Briefly, these schemes equip the sensors with scalar states that iteratively merge the measurements and the state of other sensors to be used for state estimation. Contrarily to commonly used distributed estimation schemes, the only information being exchanged are scalars, there is only one common time-scale for communication and estimation, and the retrieval of the state of the system and sensors is achieved in finite-time. We extend previous work to a more general setup and provide necessary and sufficient conditions required for the communication between the sensors that enable the use of limited communication decentralized estimation~schemes. Additionally, we discuss the cases where the sensors are memoryless, and where the sensors might not have the capacity to discern the contributions of other sensors. Based on these conditions and the fact that communication channels incur a cost, we cast the problem of finding the minimum cost communication graph that enables limited communication decentralized estimation schemes as an integer programming problem. "
1801_06779.tex," 		In this paper, we construct three families of semigroup algebras using Puiseux monoids, recently-studied additive submonoids of $\qq$ . The first family consists of semigroup algebras of certain atomic Puiseux monoids called primary. Properties of primary Puiseux monoids will allow us to show that the semigroup algebras they determine are atomic. The second family of algebras is constructed over a given algebraically closed field by using integrally closed Puiseux monoids. As we shall prove, the algebraically closedness of the field and the integrally closedness of the monoids ensure that the semigroup algebras they determine are antimatter Bezout domains. Then, we use a class of divisible Puiseux monoids to construct our last family of antimatter semigroup algebras over any perfect field of finite characteristic. 	"
1801_10567_abstractPCA.tex," Sparse principal component analysis (sPCA) has become one of the most widely used techniques for dimensionality reduction in high-dimen \-sio \-nal datasets. The main challenge underlying sPCA is to estimate the first vector of loadings of the population covariance matrix, provided that only a certain number of loadings are non-zero. In this paper, we propose confidence intervals for individual loadings and for the largest eigenvalue of the population covariance matrix. %study asymptotically normal estimation of eigenstructure of covariance matrices in high-dimensional regimes. Our results have direct implications to important statistical problems such as principal component analysis and testing for the covariance structure of high-dimensional data. Given an independent sample $\Xobsi\in\mathbb R^p,i=1,\dots,n$ , generated from an unknown distribution with an unknown covariance matrix $\Sigma_0$ , % with zero-mean rows % we denote the Gram matrix by $\hat\Sigma := X^T X/n$ and the population covariance matrix by $\Sigma_0:=\mathbb E \hat\Sigma.$  %Estimation of the principal components in high-dimensional settings is plagued by non-convexity. we study estimation of the first vector of {loadings}  %the covariance matrix $\Sigma_0:=\mathbb E X^TX /n$ in a setting where $p\gg n$ . %Namely we concentrate on estimation of the first vector of loadings . % We aim to provide estimators for the first loadings vector % %intro.spca %arg\beta\in\mathbb R^p 1{4}\|\Sigma_0- \beta\beta^T\|_F^2, % %where $\|\cdot\|_F$ is the Frobenius norm of a matrix. %The first vector of loadings determines the best rank-one approximation to $\Sigma_0.$ % $\beta_0$ is the eigenvector of $\Sigma_0$ corresponding to its maximum eigenvalue $\max$ , scaled to $\max.$  Next to the high-dimensio \-nality, another challenge lies in the inherent non-convexity of the problem. We base our methodology on a Lasso-penalized M-estimator which, despite non-convexity, may be solved by a polynomial-time algorithm such as coordinate or gradient descent. We show that our estimator achieves the minimax optimal rates in $\ell_1$ and $\ell_2$ -norm. % %spca.overview %argmin_{1{4}\|\hat\Sigma-\beta\beta^T\|_F^2 + \lambda\|\beta\|_1, % %where the set $\mathcal B$ is a local set obtained from an initial rough estimator and $\|\cdot\|_1$ is the $\ell_1$ -norm. %Localization is used to guarantee convexity of the population risk function. The empirical risk function might still be non-convex, but %To overcome non-convexity, we provide oracle inequalities for any stationary point $\hat\beta$ of the program spca.overview . %The estimator is not asymptotically normal due to bias, but We identify the bias in the Lasso-based estimator and propose a de-biased sparse PCA estimator for the vector of loadings and for the largest eigenvalue of the covariance matrix $\Sigma_0$ . Our main results provide theoretical guarantees for asymptotic normality of the de-biased estimator. %construction of confidence intervals; The major conditions we impose are sparsity in the first eigenvector of small order $n/\log p$ and sparsity of the same order in the columns of the inverse Hessian matrix of the population risk. \vskip 0.2cm \noindent % %62J07 (62F12). %, 62F30, (62H12, 90C25) %  \noindent 	covariance matrix, eigenvectors, eigenvalues, PCA, high-dimensional model, sparsity, Lasso,  asymptotic normality, confidence intervals. 	 	 "
1801_04484.tex," We investigate a conjecture about stabilisation of deficiency in finite index subgroups and relate it to the D2 Problem of C.T.C. Wall and the Relation Gap problem. We verify the pro- $p$ version of the conjecture, as well as its higher dimensional abstract analogues. "
1801_03374.tex," In this paper, we introduce a new notion of biprojectivity, called $WAP$ -biprojective for $F(A)$ , the enveloping dual Banach algebra associated to a Banach algebra $A$ . We study the relation between this new notion to Connes biprojectivity and Connes amenability and we conclude that, for a given dual Banach algebra $A$ , if $F(A)$ is Connes amenable, then $A$ is Connes amenable.  We prove that for a locally compact group $G,$  $F(L^1(G))$ is $WAP$ -biprojective if and only if $G$ is amenable. Also for an infinite commutative compact group $G$ , we show that the convolution Banach algebra $F(L^2(G))$ is not $WAP$ -biprojective. Finally, we provide some examples of the enveloping dual Banach algebras associated to the certain Banach algebras and we study its $WAP$ -biprojectivity and Connes amenability. "
1801_05881_sample-sigconf.tex," Due to instant availability of data on social media platforms like Twitter, and advances in machine learning and data management technology, real-time crisis informatics has emerged as a prolific research area in the last decade. Although several benchmarks are now available, especially on portals like CrisisLex, an important, practical problem that has not been addressed thus far is the rapid acquisition and benchmarking of data from free, publicly available streams like the Twitter API. In this paper, we present ongoing work on a pipeline for facilitating immediate post-crisis data collection, curation and relevance filtering from the Twitter API. The pipeline is minimally supervised , alleviating the need for feature engineering by including a judicious mix of data preprocessing and fast text embeddings, along with an active learning framework. We illustrate the utility of the pipeline by describing a recent case study wherein it was used to collect and analyze millions of tweets in the immediate aftermath of the Las Vegas shootings.  "
1801_08166_ms4.tex," We present Korea Microlensing Telescope Network (KMTNet) light curves for microlensing-event candidates in the {\it Kepler} K2 C9 field having peaks within 3 effective timescales of the {\it Kepler} observations. These include 181 ``clear microlensing'' and 84 ``possible microlensing'' events found by the KMTNet event finder, plus 56 other events found by OGLE and/or MOA that were not found by KMTNet. All data for the first two classes are immediately available for public use without restriction. "
1801_06410.tex," We study a cohomology theory $H^{\ph$ , called the $\mathcal L_B$ -cohomology, on compact torsion-free $\G$ -manifolds. We show that $H^k_{\DR$ for $k \neq 3, 4$ , but that $H^k_{\ph}$ is infinite-dimensional for $k = 3,4$ . Nevertheless there is a canonical injection $H^k_{\ph$ . The $\mathcal L_B$ -cohomology also satisfies a Poincar \'e duality induced by the Hodge star. The establishment of these results requires a delicate analysis of the interplay between the exterior derivative $\dd$ and the derivation $\mathcal L_B$ , and uses both Hodge theory and the special properties of $\G$ -structures in an essential way. As an application of our results, we prove that compact torsion-free $\G$ -manifolds are `almost formal' in the sense that most of the Massey triple products necessarily must vanish. "
1801_03615_draft.tex," Neural machine translation (NMT) suffers a performance deficiency when a limited vocabulary fails to cover the source or target side adequately, which happens frequently when dealing with morphologically rich languages. To address this problem, previous work focused on adjusting translation granularity or expanding the vocabulary size. However, morphological information is relatively under-considered in NMT architectures, which may further improve translation quality. We propose a novel method, which can not only reduce data sparsity but also model morphology through a simple but effective mechanism. By predicting the stem and suffix separately during decoding, our system achieves an improvement of up to 1.98 BLEU compared with previous work on English to Russian translation. Our method is orthogonal to different NMT architectures and stably gains improvements on various domains. "
1801_06469_main_text.tex," 	We present an inductively coupled-plasma reactive-ion etching process that simultaneously provides both a high etch rate and unprecedented selectivity for gallium phosphide (GaP) in the presence of aluminum gallium phosphide (Al $_x$ Ga $_{1-x}$ P). Utilizing mixtures of silicon tetrachloride (SiCl $_4$ ) and sulfur hexafluoride (SF $_6$ ), selectivities exceeding 2700:1 are achieved at GaP etch rates above 3000 nm/min. A design of experiments has been employed to investigate the influence of the inductively coupled-plasma power, the chamber pressure, the DC bias and the ratio of SiCl $_4$ to SF $_6$ . The process enables the use of thin Al $_x$ Ga $_{1-x}$ P stop layers even at aluminum contents of a few percent. "
1801_04669_ft-hotelling-game-full.tex," The $n$ -player Hotelling game calls for each player to choose a point on the line segment, so as to maximize the size of his Voronoi cell. This paper studies fault-tolerant versions of the Hotelling game. Two fault models are \commful studied. The first \commfulend \commabs studied: line faults and player faults. The first model \commabsend assumes that the environment is prone to failure: with some probability, a disconnection occurs at a random point on the line, splitting it into two separate segments and modifying each player's Voronoi cell accordingly. A complete characterization of the Nash equilibria of this variant is provided for every $n$ . Additionally, a one to one correspondence is shown between equilibria of this variant and of the Hotelling game with no faults. The second fault model assumes the players are prone to failure: each player is removed from the game with i.i.d. probability, changing the payoffs of the remaining players accordingly. It is shown that for $n \geq 3$ this variant of the game has no Nash equilibria. %DP: This should be said in the intro, not the abstract. %Fault-tolerant problems constitute a fertile area of research in Computer Science. This paper is the first to consider the well known Hotelling game through the lens of fault tolerance.  \commful Hotelling game, fault-tolerant games, competitive location problems.  \commfulend"
1801_00716_main-TR-final.tex,"  Given a hypergraph $H = (V,E)$ , what is the smallest subset $X  \subseteq V$ such that $e \cap X \neq \emptyset$ holds for all $e \in E$ ?  This problem, known as the hitting set problem, is a  basic problem in parameterized complexity theory. There are well-known  kernelization algorithms for it, which get a hypergraph~ $H$ and a  number~ $k$ as input and output a hypergraph~ $H'$ such that (1)  $H$ has a hitting set of size~ $k$ if, and only if, $H'$ has such a  hitting set and (2) the size of $H'$ depends only on $k$  and on the maximum cardinality $d$ of edges in~ $H$ . The  algorithms run in polynomial time, but are highly  sequential. Recently, it has been shown that one of them can be parallelized  to a certain degree: one can compute hitting set kernels in parallel  time $O(d)$ -- but it was conjectured that this is  the best parallel algorithm possible. We  refute this conjecture and show how hitting set kernels can be  computed in constant parallel time. For our proof, we  introduce a new, generalized notion of hypergraph sunflowers and  show how iterated applications of the color coding technique can  sometimes be collapsed into a single application. "
1801_05433_ComBinE.tex,"  The first gravitational wave detections of mergers between black holes and neutron stars represent a remarkable new regime of high-energy transient astrophysics. The signals observed with LIGO-Virgo detectors come from mergers of extreme physical objects which are the end products of stellar evolution within close binary systems. To better understand their origin and merger rates, we have performed binary population syntheses at different metallicities using the new stellar grid based populations synthesis code 3.0{-1}$ for Milky-Way equivalent galaxies. Our absolute upper limit to the merger-rate density of double neutron star systems is $R\simeq\unit{400{-1\rm pc^{-3}}$} in the local Universe ( $z=0$ ). "
1801_09519_The_Lazy_bootstrap.tex,"\abstract{The latent class model is a powerful unsupervised clustering algorithm for categorical data. Many statistics exist to test the fit of the latent class model. However, traditional methods to evaluate those fit statistics are not always useful. Asymptotic distributions are not always known, and empirical reference distributions can be very time consuming to obtain. In this paper we propose a fast resampling scheme with which any type of model fit can be assessed. We illustrate it here on the latent class model, but the methodology can be applied in any situation. 	 	The principle behind the lazy bootstrap method is to specify a statistic which captures the characteristics of the data that a model should capture correctly. If those characteristics in the observed data and in model-generated data are very different we can assume that the model could not have produced the observed data. With this method we achieve the flexibility of tests from the Bayesian framework, while only needing maximum likelihood estimates. We provide a step-wise algorithm with which the fit of a model can be assessed based on the characteristics we as researcher find important. In a Monte Carlo study we show that the method has very low type I errors, for all illustrated statistics. Power to reject a model depended largely on the type of statistic that was used and on sample size. We applied the method to an empirical data set on clinical subgroups with risk of Myocardial infarction and compared the results directly to the parametric bootstrap. The results of our method were highly similar to those obtained by the parametric bootstrap, while the required computations differed three orders of magnitude in favour of our method.}"
1801_05517_Takeuchi_QG4.tex," Domain size distribution in phase separating binary Bose--Einstein condensates is studied theoretically by numerically solving the Gross--Pitaevskii equations at zero temperature. We show that the size distribution in the domain patterns arising from the dynamic instability obeys a power law in a scaling regime according to the dynamic scaling analysis based on the percolation theory. The scaling behavior is kept during the relaxation development until the characteristic domain size becomes comparable to the linear size of the system, consistent with the dynamic scaling hypothesis of the phase-ordering kinetics. Our numerical experiments indicate the existence of a different scaling regime in the size distribution function, which can be caused by the so-called coreless vortices.  Bose--Einstein condensates, phase separation, percolation "
1801_07572_Clark_Probing_the_limits_DPC-STEM.tex," The rigid-intensity-shift model of differential phase contrast scanning transmission electron microscopy (DPC-STEM) imaging assumes that %One simple model of differential phase contrast scanning transmission electron microscopy (DPC-STEM) imaging posits that the phase gradient imposed on the probe by the sample causes the diffraction pattern intensity to shift rigidly by an amount proportional to that phase gradient. This % idealisation behaviour is seldom realised exactly in practice. Through a combination of experimental results, analytical modelling and numerical calculations, we explore the breakdown of the rigid-intensity-shift behaviour and how this depends on the magnitude of the phase gradient and the relative scale of features in the phase profile and the probe size. We present guidelines as to when the rigid-intensity-shift model can be applied for quantitative phase reconstruction using segmented detectors, and propose probe-shaping strategies to further improve the accuracy. "
1801_06530_main_v2_arxiv.tex, The ability of detecting threatening chemicals diluted in water is an important safety requirement for drinking water systems. An apparatus for in-water chemical sensing based on the absorption of evanescent waves generated by a quantum cascade laser array and propagating inside a silver halide optical fiber immersed into water is demonstrated. We present a theoretical analysis of the sensitivity of the system and experimentally characterize its real-time response and spectroscopic detection for injection of a sample chemical (ethanol) in a tube containing water.
1801_00210.tex," We study analytic and arithmetic properties of the elliptic gamma function $$ m,n=0^1-x^{-1q^{m+1}p^{n+1}}{1-xq^mp^n}, \qquad |q|,|p|<1, $$ in the regime $p=q$ ; in particular, its connection with the elliptic dilogarithm and a formula of S.~Bloch. We further extend the results to more general products by linking them to non-holomorphic Eistenstein series and, via some formulae of D.~Zagier, to elliptic polylogarithms. "
1801_05631_SC_QCP_sim.tex," When a metal undergoes continuous quantum phase transition, the correlation length diverges at the critical point and the quantum fluctuation of order parameter is described by a gapless bosonic mode. In most cases, this bosonic mode induces a variety of unusual quantum critical phenomena, including non-Fermi liquid behavior and various emergent symmetries. Here, we perform a renormalization group analysis of the semimetal-superconductor quantum criticality in a three-dimensional anisotropic Weyl semimetal, in which the fermion dispersion is linear in two of the momentum components and quadratical in the third. Unexpectedly, distinct from previously studied quantum critical systems, the anisotropic Weyl fermions do not acquire an anomalous dimension at the critical point, and their quasiparticle residue takes a nonzero value. This indicates that the fermions can be well described by the model of non-interacting fermion gas even at the quantum critical point. We thus obtain a phase transition that exhibits trivial quantum criticality, which is unique comparing to other invariably nontrivial quantum critical systems. "
1801_07647_main.tex, abstract.txt
1801_04763_Gravitational_wave_detection_via_weak_measurements.tex," A new gravitational-wave detector, which is devised based on quantum weak measurement amplification, is introduced and shown has the potential to significantly improve the strain sensitivity of gravitational-wave detection. "
1801_09581_emffs_disc.tex,"  We evaluate the strange nucleon electromagnetic form factors using an  ensemble of gauge configurations generated with two degenerate maximally twisted mass clover-improved  fermions with mass tuned to approximately reproduce the physical pion mass. In addition, we present results for the disconnected light quark  contributions to the nucleon electromagnetic form factors. Improved stochastic methods are employed leading to high-precision results. The momentum dependence of the disconnected contributions is fitted using the model-independent z-expansion. We  extract the magnetic moment and the  electric and magnetic radii of the proton and neutron by including both connected and disconnected contributions. We find that the disconnected light quark contributions to both electric and magnetic form factors are non-zero and at the few percent level as compared to the connected. The strange form factors are also at the percent level but more noisy yielding statistical errors that are typically within one standard deviation from a zero value.  "
1801_09522_bare_conf.tex," In this paper, we propose a stacked convolutional and recurrent neural network (CRNN) with a 3D convolutional neural network (CNN) in the first layer for the multichannel sound event detection (SED) task. The 3D CNN enables the network to simultaneously learn the inter- and intra-channel features from the input multichannel audio. In order to evaluate the proposed method, multichannel audio datasets with different number of overlapping sound sources are synthesized. Each of this dataset has a four-channel first-order Ambisonic, binaural, and single-channel versions, on which the performance of SED using the proposed method are compared to study the potential of SED using multichannel audio. A similar study is also done with the binaural and single-channel versions of the real-life recording TUT-SED 2017 development dataset. The proposed method learns to recognize overlapping sound events from multichannel features faster and performs better SED with a fewer number of training epochs. The results show that on using multichannel Ambisonic audio in place of single-channel audio we improve the overall F-score by 7.5~ \%, overall error rate by 10~ \% and recognize 15.6~ \% more sound events in time frames with four overlapping sound sources. % In this paper, we assess the performance of using multichannel audio in place of single-channel audio for polyphonic sound event detection (SED). Multichannel audio datasets with different number of overlapping sound sources are synthesized, where each dataset has a four-channel first-order Ambisonic, binaural, and single-channel versions. The proposed stacked convolutional and recurrent neural network with a 3D convolutional layer is trained individually on the synthesized multichannel datasets. This 3D convolution enables the network to simultaneously learn the inter- and intra-channel features from multichannel input data. The SED performances with single-channel, binaural and Ambisonic audio are further compared to study the potential of SED using multichannel audio. A similar study is also done with the binaural and single-channel versions of the real-life recording TUT-SED 2017 development dataset. The results show that on using multichannel Ambisonic audio in place of single-channel audio we improve the overall F-score by 7.5~ \%, overall error rate by 10~ \% and recognize 15.6~ \% more sound events in time frames with four overlapping sound sources. "
1801_09344_abstract.tex," While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this work, we study this problem for neural networks with one hidden layer. We first propose a method based on a semidefinite relaxation  that outputs a certificate that for a given network and test input,  no attack can force the error to exceed a certain value. Second, as this certificate is differentiable, we jointly optimize it  with the network parameters, providing an adaptive regularizer that  encourages robustness against all attacks.  On MNIST, our approach produces a network and a certificate that no attack that perturbs each pixel by  at most $\epsilon = 0.1$ can cause more than $35\%$ test error. "
1801_10352_RMP_EE.tex," Entanglement entropy plays a variety of roles in quantum field theory, including the connections between quantum states and gravitation through the holographic principle. This article provides a review of entanglement entropy from a mixed viewpoint of field theory and holography. A set of basic methods for the computation is developed and illustrated with simple examples such as free theories and conformal field theories. The structures of the ultraviolet divergences and the universal parts are determined and compared with the holographic descriptions of entanglement entropy. The utility of quantum inequalities of entanglement are discussed and shown to derive the $\CC$ -theorem that constrains renormalization group flows of quantum field theories in diverse dimensions. "
1801_09463_DQPT_SB.tex," We study the dynamical quantum phase transitions (DQPTs) manifested in the subsequent unitary dynamics of an extended Ising model with {additional} three spin interactions {, following a sudden quench} . Revisiting the equilibrium phase diagram of the model where different quantum phases are characterised by different winding numbers, we show {that in some situations the winding number may not change across a gap closing point in the energy spectrum} . Although, usually there exist {s} a one-to-one correspondence between the change in winding number and the number of critical time scales {associated with DQPTs} , we show that the extended nature of interactions may lead to unusual situations. % % \item[Usage] %Secondary publications and information retrieval purposes. % \item[PACS numbers] %May be entered using the \verb+ #1 + command. % \item[Structure] %You may use the description environment to structure your abstract; %use the optional argument of the \verb+ \item+ command to give the category of each item. % "
1801_08497_Pigulski-poster-SigS-arXiv.tex," Preliminary results of the analysis of the combined space-based BRITE and SMEI, and ground-based Str {\""o} mgren photometry are presented. The BRITE data allowed to find seven $p$ and three $g$ modes in the frequency spectrum of this star; only four $p$ modes were known in this star prior to this study. The first results of seismic modelling are also presented. "
1801_02695_tsp_in37_n.tex," Consider~ \(n \) nodes~ X_i\_{1 \leq i \leq n} \) distributed independently across~ \(N \) cities contained with the unit square~ \(S \) according to a distribution~ \(f. \) Each city is modelled as an~ \(r_n \times r_n \) square contained within~ \(S \) and let~ \(TSPC_n \) denote the length of the minimum length cycle containing all the~ \(n \) nodes, corresponding to the traveling salesman problem (TSP). We obtain variance estimates for~ \(TSPC_n \) and prove that if the cities are well-connected and densely populated in a certain sense, then~ \(TSPC_n \) appropriately centred and scaled converges to zero in probability. We also obtain large deviation type estimates for~ \(TSPC_n. \) Using the proof techniques, we alternately obtain corresponding results for the length~ \(TSP_n \) of the minimum length cycle in the unconstrained case, when the nodes are independently distributed throughout the unit square~ \(S. \)    %and analogous results hold for minimum spanning trees and paths.  %In this paper, we study the structure of left-right crossings %of the random geometric graph \(G = G(n,r_n) \) of \(n \) nodes %uniformly distributed in \(S[0,1] = ^2 \) with \(r_n = \epsilon \frac{\log{n}{n}} \) %for some \( \epsilon > 0. \) Tiling \(S \) horizontally and %vertically into rectangles of length \(1 \) and width \(Mr_n, \) we %show that each rectangle has a left-right crossing of edges with %high probability if \(M \) is sufficiently large. %We call the resulting subgraph to be a ``backbone"" of \(G. \) %The techniques we use to construct the backbone has quite a few applications. %As a first, we show that the diameter of second largest component in \(G \) %is \(O(1) \) with high probability. Secondly, 0.1in  \noindent Key words: Traveling salesman problem, dense cities.  0.1in  \noindent AMS 2000 Subject Classification: Primary: 60J10, 60K35; Secondary: 60C05, 62E10, 90B15, 91D30. "
1801_04348_abstract.tex," This work deals with the optimization of computer programs targeting Graphics Processing Units (GPUs). The goal is to lift, from programmers to optimizing compilers, the heavy burden of determining program details that are dependent on the hardware characteristics. The expected benefit is to improve robustness, portability and efficiency of the generated computer programs. We address these requirements by:   \item treating machine and program parameters as unknown symbols during code generation, and \item generating optimized programs in the form of a case discussion, based on the possible values of the machine and program parameters. By taking advantage of recent advances in the area of computer algebra, preliminary experimentation yield promising results. "
1801_07760.tex," Let $R$ be a commutative Noetherian ring, $\fa$ be an ideal of $R$ and $M$ be an $R$ -module. It is shown that if $\Ext^i_R(R/\fa,M)$ is minimax for all $i\leq \dim M$ , then the $R$ -module $\Ext^i_R(N,M)$ is minimax for all $i\geq 0$ and for any finitely generated $R$ -module $N$ with $\Supp_R(N) \subseteq V (\fa)$ and $\dim N \leq 1$ . As a consequence of this result we obtain that for any $\fa$ -torsion $R$ -module $M$ that $\Ext^i_R(R/\fa, M)$ is minimax for all $i\leq \dim M$ , all Bass numbers and all Betti numbers of $M$ are finite. This generalizes BNS2015 . Also, some equivalent conditions for the cominimaxness of local cohomology modules with respect to ideals of dimension at most one are given. "
1801_03301_Cr_surface_electron_phonon_coupling.tex," It is experimentally well established that the Cr(001)-surface exhibits a sharp resonance around the Fermi level. However, there is no consensus about its physical origin. It is proposed to be either due to a single particle $d_{z^{2}$} surface state renormalised by electron-phonon coupling or the orbital Kondo effect involving the degenerate $d_{xz$} / $d_{yz$} states. In this work we examine the electron-phonon coupling of the Cr(001)-surface by means of ab-initio calculations in the form of density functional perturbation theory. More precisely, the electron-phonon mass-enhancement factor of the surface layer is investigated for the 3d states. For the majority and minority spin $d_{z^{2}$} surface states we find values of $0.19$ and $0.16$ . We show that these calculated electron-phonon mass-enhancement factors are not in agreement with the experimental data even if we use realistic values for the temperature range and surface Debye frequency for the fit of the experimental data. More precisely, then experimentally an electron-phonon mass-enhancement factor of $0.70\pm0.10$ is obtained, which is not in agreement with our calculated values of $0.19$ and $0.16$ . Therefore, we conclude that the experimentally observed resonance at the Cr(001)-surface is not due to polaronic effects, but due to electron-electron correlation effects.  "
1801_07212_hcssh.tex,"A better understanding of sulphur chemistry is needed to solve the interstellar sulphur depletion problem. A way to achieve this goal is to study new S-bearing molecules in the laboratory, obtaining accurate rest frequencies for an astronomical search. We focus on dithioformic acid, HCSSH, which is the sulphur analogue of formic acid. % % context (optional) "
1801_00702_manuscript.tex," As a generalization of Dempster-Shafer theory, the theory of D numbers is a new theoretical framework for uncertainty reasoning. Measuring the uncertainty of knowledge or information represented by D numbers is an unsolved issue in that theory. In this paper, inspired by distance based uncertainty measures for Dempster-Shafer theory, a total uncertainty measure for a D number is proposed based on its belief intervals. The proposed total uncertainty measure can simultaneously capture the discord, and non-specificity, and non-exclusiveness involved in D numbers. And some basic properties of this total uncertainty measure, including range, monotonicity, generalized set consistency, are also presented. "
1801_09081.tex," In this paper, we pose several conjectures on structures and images of maximal rationally connected fibrations of smooth projective varieties admitting semi-positive holomorphic sectional curvature. Toward these conjectures, we prove that the numerical dimension of images of such fibrations is zero under the assumption of the abundance conjecture. As an application, we show that any compact K \""ahler surface with semi-positive holomorphic sectional curvature is rationally connected, or a complex torus, or a ruled surface over an elliptic curve. "
1801_00970_Topological_G-Operads.tex, This paper shows that the semi-direct product construction for $G$ -operads and the levelwise Borel construction for $G$ -cooperads are intertwined by the topological operadic bar construction.
1801_06408_main.tex," In query optimisation accurate cardinality estimation is essential for finding optimal query plans. It is especially challenging for RDF due to the lack of explicit schema and the excessive occurrence of joins in RDF queries. Existing approaches typically collect statistics based on the counts of triples and estimate the cardinality of a query as the product of its join components, where errors can accumulate even when the estimation of each component is accurate. As opposed to existing methods, we propose PRESTO, a cardinality estimation method that is based on the counts of subgraphs instead of triples and uses a probabilistic method to estimate cardinalities of RDF queries as a whole. PRESTO avoids some major issues of existing approaches and is able to accurately estimate arbitrary queries under a bound memory constraint. We evaluate PRESTO with YAGO and show that PRESTO is more accurate for both simple and complex queries. "
1801_02344_Draft.tex," This letter introduces a novel wireless-powered backscatter communication system which allows sensors to utilize RF signals transmitted from a dedicated RF energy source to transmit data. In the proposed system, when the RF energy source transmits RF signals, the sensors are able to backscatter the RF signals to transmit date to the gateway and/or harvest energy from the RF signals for their operations. By integrating backscattering and energy harvesting techniques, we can optimize the network throughput of the system. In particular, we first formulate the time scheduling problem for the system, and then propose an optimal solution using convex optimization to maximize the overall network throughput. Numerical results show a significant throughput gain achieved by our proposed design over two other baseline schemes. "
1801_04476.tex," We prove that any affine ring $R$ over a field $k$ has full-cosupport, {\it i.e.} , the cosupport of $R$ is equal to $\Spec R$ . Using this fact, we give a complete description of all terms in a minimal pure-injective resolution of $R$ , provided that $|k|=\aleph_1$ and $\dim R\geq 2$ . As a corollary, we obtain a partial answer to a conjecture by Gruson. "
1801_04370_ms.tex," With observations of the Interface Region Imaging Spectrograph (IRIS), we study chromospheric heating and evaporation during an M1.6 flare SOL2015-03-12T11:50. At the flare ribbons, the Mg~ {ii} ~2791.59  line shows quasi-periodic short-duration red-wing enhancement, which is likely related to repetitive chromospheric condensation as a result of episodic heating. On the contrary, the Si~ {iv} ~1402.77  line reveals a persistent red-wing asymmetry in both the impulsive and decay phases, suggesting that this line responds to both cooling downflows and chromospheric condensation. The first two episodes of red-wing enhancement occurred around 11:42 UT and 11:45 UT, when two moving brightenings indicative of heating fronts crossed the IRIS slit. The greatly enhanced red wings of the Si~ {iv} ~and Mg~ {ii} ~lines at these occasions are accompanied by an obvious increase in the line intensities and the HXR flux, suggesting two episodes of energy injection into the lower atmosphere in the form of nonthermal electrons. The Mg~ {ii} ~k/h ratio has a small value of $\sim$ 1.2 at the ribbons and decreases to $\sim$ 1.1 at these two occasions. Correspondingly, the Fe~ {xxi} ~1354  ~line reveals two episodes of chromospheric evaporation, which is characterized as a smooth decrease of the blue shift from $\sim$ 300 km~s $^{-1}$ to nearly zero within $\sim$ 3 minutes. The Fe~ {xxi} ~1354  ~line is entirely blueshifted in the first episode, while appears to contain a nearly stationary component and a blueshifted component in the second episode. More episodes of blueshifted Fe~ {xxi} ~emission is found around the northern ribbon in the decay phase, though no obvious response is detected in the Si~ {iv} ~and Mg~ {ii} ~emission. We have also examined the Fe~ {xxi} emission at the flare loop top and identified a secondary component with a $\sim$ 200 km~s $^{-1}$ red shift, which possibly results from the downward moving reconnection outflow. Our analysis also suggests a reference wavelength of 1354.0878 $\pm$ 0.0072  for this Fe~ {xxi} line. "
1801_01254_main.tex," 	Spicule oscillations involve high-frequency components with a typical period approximately corresponding to $40-50$ s. 	The typical time scale of the photospheric oscillation is a few minutes, and thus, the origin of this high-frequency component is not trivial. 	In this study, a one-dimensional numerical simulation is performed to demonstrate that the observed spicule oscillations originate from longitudinal-to-transverse mode conversion 	that occurs around the equipartition layer in the chromosphere. 	Calculations are conducted in a self-consistent manner with the exception of additional heating to maintain coronal temperature. 	The analyses indicate the following features: 	(1) mode conversion efficiently excites high-frequency transverse waves; 	(2) the typical period of the high-frequency waves corresponds to the sound-crossing time of the mode conversion region; and 	(3) simulated root-mean-square velocity of the high-frequency component is consistent with the observed value. 	These results indicate that the observation of spicule oscillation provides direct evidence of mode conversion in the chromosphere. 		 "
1801_06466_exp_sc.tex," Let $G$ be a finite abelian group of order $n$ and let $\dn$ denote the $(n-1)$ -simplex on the vertex set $G$ . The sum complex  $X_{A,k}$ associated to a subset $A \subset G$ and $k < n$ , is the $k$ -dimensional simplicial complex obtained by taking the full $(k-1)$ -skeleton of $\dn$ together with all $(k+1)$ -subsets $\sigma \subset G$ that satisfy $x \in \sigma x \in A$ . Let $C^{k-1}(X_{A,k})$ denote the space of complex valued $(k-1)$ -cochains of $X_{A,k}$ . Let $L_{k-1}:C^{k-1}(X_{A,k}) k-1(X_{A,k})$ denote the reduced $(k-1)$ -th Laplacian of $X_{A,k}$ , and let $k-1(X_{A,k})$ be the minimal eigenvalue of $L_{k-1}$ . \\ It is shown that if $k \geq 1$ and $\epsilon>0$ are fixed, and $A$ is a random subset of $G$ of size $m=4k^2\log n{\epsilon^2}\rceil$ , then \[ \pr k-1(X_{A,k}) < (1-\epsilon)m~\big] =O \left(\ frac {1} {n} \right). \] \ \\ \\ 2000 MSC: 05E45, 60C05 \\ Keywords:  Random sum complexes, High dimensional Laplacians, Spectral gap. "
1801_06555.tex," We prove a theorem of Tits type for automorphism groups of projective varieties over an algebraically closed field of arbitrary characteristic, which was first conjectured by Keum, Oguiso and Zhang for complex projective varieties. "
1801_03102_306985.tex," Recent advances in the physical vapor deposition (PVD) of protective fluoride films have raised the far ultraviolet (FUV: 912 -- 1600 \AA) reflectivity of aluminum-based mirrors closer to the theoretical limit. The greatest gains, at more than 20$\%$ , have come for lithium fluoride protected aluminum (LiF+Al), which has the shortest wavelength cutoff of any conventional overcoat. Despite the success of the NASA $FUSE$ mission, the use of LiF-based optics is rare as LiF is hygroscopic and requires handling procedures that can drive risk. With NASA now studying two large mission concepts for astronomy (LUVOIR and HabEx) that mandate throughput down to 1000 \AA, the development of LiF-based coatings becomes crucial. This paper discusses steps that are being taken to qualify these new enhanced LiF protected aluminum (eLiF) mirror coatings for flight. In addition to quantifying the hygroscopic degradation, we have developed a new method of protecting eLiF with an ultrathin (10 -- 20 \AA) capping layer of a non-hygroscopic material to increase durability. We report on the performance of eLiF-based optics and assess the steps that need to be taken to qualify such coatings for LUVOIR, HabEx, and other FUV-sensitive space missions.  "
1801_09223_decomp_local_mi_inf_mis_contrib.tex,"  The pointwise mutual information quantifies the mutual information between events $x$ and $y$ from  random variable $X$ and $Y$ . This article considers the pointwise mutual information in a  directed sense, examining precisely how an event $y$ provides information about $x$ via  probability mass exclusions. Two distinct types of exclusions are identified---namely informative  and misinformative exclusions. Then, inspired by Fano's postulates for the pointwise mutual  information, three postulates are proposed that aim to decompose the pointwise mutual information  into a non-negative informational component associated with the informative exclusions, and a  non-positive informational component associated with the misinformative exclusions. This leads to  a novel derivation of a familiar decomposition of the pointwise mutual information into its  entropic components. The paper concludes by discussing the relevance of considering information  in terms of probability mass exclusions to the ongoing effort to decompose multivariate  information. "
1801_08839_GeoGAN.tex," 	Instance segmentation is a problem of significance in computer vision. However, preparing annotated data for this task is extremely time-consuming and costly. By combining the advantages of 3D scanning, physical reasoning, and GAN techniques, we introduce a novel pipeline named Geometry-guided GAN (GeoGAN) to obtain large quantities of training samples with minor annotation. Our pipeline is well-suited to most indoor and some outdoor scenarios. To evaluate our performance, we build a new Instance-60K dataset, with various of common objects categories. Extensive experiments show that our pipeline can achieve decent instance segmentation performance given very low human annotation cost. "
1801_02789_manuscript3.tex," Secure communication is a matter of genuine concern that includes means whereby entities can share information without a third party's interception. Key agreement protocols are one of the common approaches in which two or more parties can agree upon a key, which precludes undesired third parties from forcing a key choice on them. Over the past decade, chaos-based key agreement protocols have been studied and employed widely. Recently, Yoon and Jeon proposed a novel key agreement protocol based on chaotic maps and claimed security and practicality for their protocol. We find that Yoon-Jeon's protocol suffers certain issues: (1) It introduces a trusted third party whose very presence increases the implementation cost. (2) requires a multiplicity of encryption/decryption computations and (3) does not protect the user's anonymity. In order to overcome these problems, we present an enhanced key agreement protocol with user anonymity. Theoretical analysis demonstrates that the proposed protocol is efficient and resists current attacks.  "
1801_08082_mgb17v2.tex," A kinetic equation for a system of elastic hard spheres or disks confined by a hard wall of arbitrary shape is derived. It is a generalization of the modified Enskog equation in which the effects of the confinement are taken into account and it is supposed to be valid up to moderate densities. From the equation, balance equations for the hydrodynamic fields are derived, identifying the collisional transfer contributions to the pressure tensor and heat flux. A Lyapunov functional, $H[f]$ , is identified. For any solution of the kinetic equation, $H$ decays monotonically in time until the system reaches the inhomogeneous equilibrium distribution, that is a Maxwellian distribution with a the density field consistent with equilibrium statistical mechanics.  \keywords{Kinetic theory \and hard-sphere fluid \and Enskog equation \and $H$-theorem} "
1801_03329_paper.tex," We consider the task of weakly supervised one-shot detection. In this task, we attempt to perform a detection task over a set of unseen classes, when training only using weak binary labels that indicate the existence of a class instance in a given example. The model is conditioned on a single exemplar of an unseen class and a target example that may or may not contain an instance of the same class as the exemplar. A similarity map is computed by using a Siamese neural network to map the exemplar and regions of the target example to a latent representation space and then computing cosine similarity scores between representations. An attention mechanism weights different regions in the target example, and enables learning of the one-shot detection task using the weaker labels alone. The model can be applied to detection tasks from different domains, including computer vision object detection. We evaluate our attention Siamese networks on a one-shot detection task from the audio domain, where it detects audio keywords in spoken utterances. Our model considerably outperforms a baseline approach and yields a 42.6 \% average precision for detection across 10 unseen classes. Moreover, architectural developments from computer vision object detection models such as a region proposal network can be incorporated into the model architecture, and results show that performance is expected to improve by doing so. "
1801_10023_main.tex," We review a series of quantum memory protocols designed to store the {quantum} information carried by {light} into atomic ensembles. {In particular, we show how a simple semiclassical formalism allows to gain insight into various memory protocols and to highlight strong analogies between them. These analogies naturally lead to a classification of light storage protocols in{to} two categories, {namely} photon echo and {\it slow-light} memories.}  {We focus on the storage and retrieval dynamics as a key step to map the optical information into the atomic excitation.} We finally {review} various criteria adapted for both continuous variables and photon-counting measurement techniques to {certify} the quantum nature of {these} memory {protocols} . "
1801_04281_title-LHCb-PAPER.tex,"  \noindent The \herschel detector consists of a set of scintillating counters, designed to increase the coverage of the \lhcb experiment in the high-rapidity regions on either side of the main spectrometer. The new detector improves the capabilities of \lhcb for studies of diffractive interactions, most notably Central Exclusive Production. In this paper the construction, installation, commissioning, and performance of \herschel are presented.  "
1801_08937_pcg_paper.tex,An estimation of the sky signal from streams of Time Ordered Data (TOD) acquired by the Cosmic Microwave Background (\cmb) experiments is one of the most important steps in the context of \cmb data analysis referred to as the map-making problem. The continuously growing \cmb data sets render the \cmb map-making problem progressively more challenging in terms of computational cost and memory in particular in the context of ground based experiments with their operational limitations as well as the presence of contaminants.
1801_08652_paper.tex," Recent advances in the development of commercial quantum annealers such as the D-Wave 2X allow solving NP-hard optimization problems that can be expressed as quadratic unconstrained binary programs. However, the relatively small number of available qubits (around 1000 for the D-Wave 2X quantum annealer) poses a severe limitation to the range of problems that can be solved. This paper explores the suitability of preprocessing methods for reducing the sizes of the input programs and thereby the number of qubits required for their solution on quantum computers. Such methods allow us to determine the value of certain variables that hold in either any optimal solution (called strong persistencies) or in at least one optimal solution (weak persistencies). We investigate preprocessing methods for two important NP-hard graph problems, the computation of a maximum clique and a maximum cut in a graph. We show that the identification of strong and weak persistencies for those two optimization problems is very instance-specific, but can lead to substantial reductions in the number of variables. "
1801_08496_Pigulski-Cook-arXiv.tex, The raw BRITE photometry is affected by the presence of many outliers and instrumental effects. We present and discuss possible ways to correct the photometry for instrumental effects. Special attention is paid to the procedure of decorrelation which enables removal of most of the instrumental effects and considerably improves the quality of the final photometry.
1801_07382.tex," We consider the 3D axisymmetric Euler equations without swirl on some bounded axial symmetric domains. In this setting, well-posedness is well known due to the essentially 2D geometry. The quantity $\omega^\theta/r$ plays the role of vorticity in 2D. First, we prove that the gradient of $\omega^\theta/r$ can grow at most double exponentially with improving a priori bound close to the axis of symmetry. Next, on the unit ball, we show that at the boundary, one can achieve double exponential growth of the gradient of $\omega^\theta/r$ . "
1801_09866_Template.tex," This paper presents methods to accelerate recurrent neural network based language models (RNNLMs) for online speech recognition systems. Firstly, a lossy compression of the past hidden layer outputs (history vector) with caching is introduced in order to reduce the number of LM queries. Next, RNNLM computations are deployed in a CPU-GPU hybrid manner, which computes each layer of the model on a more advantageous platform. The added overhead by data exchanges between CPU and GPU is compensated through a frame-wise batching strategy. The performance of the proposed methods evaluated on LibriSpeech http://www.openslr.org/12/ test sets indicates that the reduction in history vector precision improves the average recognition speed by 1.23 times with minimum degradation in accuracy. On the other hand, the CPU-GPU hybrid parallelization enables RNNLM based real-time recognition with a four times improvement in speed. "
1801_04403_main.tex," It has been extensively shown in past literature that Bayesian Game Theory and Quantum Non-locality have strong ties between them. Pure Entangled States have been used, in both common and conflict interest games, to gain advantageous payoffs, both at the individual and social level. In this paper we construct a game for a Mixed Entangled State such that this state gives higher payoffs than classically possible, both at the individual level and the social level. Also, we use the I-3322 inequality so that states that aren't helpful as advice for Bell-CHSH inequality can also be used. Finally, the measurement setting we use is a Restricted Social Welfare Strategy (given this particular state). "
1801_01468_sixDofSeismometer.tex," We present a novel inertial-isolation scheme based on six degree-of-freedom (6D) interferometric readout of a single reference mass. It is capable of reducing inertial motion by more than two orders of magnitude at 100 \,mHz compared with what is achievable with state-of-the-art seismometers. This would in turn dramatically improve the low-frequency sensitivity of gravitational-wave detectors. The scheme is inherently two-stage, the reference mass is softly suspended within the platform to be isolated, which is itself suspended from the ground. The platform is held constant relative to the reference mass and this closed-loop control effectively transfers the low force-noise of the reference mass to the platform. The loop gain also reduces non-linear couplings and dynamic range requirements in the soft-suspension mechanics and the interferometric readout. "
1801_01199_cochranmckay.tex," We report on imaging and spectroscopic observations of comet C/2016~R2 (Pan-STARRS) obtained with the 0.8 \,m and 2.7 \,m telescopes of McDonald Observatory in November and December 2017 respectively. The comet was at a heliocentric distance greater than 3 \sc{\sc au} during both sets of observations. The images showed a well-developed tail with properties that suggested it was an ion tail. The spectra confirmed that we were observing well-developed bands of CO $^+$ and N $_2^+$ . The N $_2^+$ detection was unequivocally cometary and was one of the strongest bands of N $_2^+$ detected in a comet spectrum. We derived the ratio of these two ions and from that we were able to derive that N $_2$ /CO = 0.15. This is the highest such ratio reported for a comet. "
1801_04294_cnt_metal_revised_final_bib.tex," Capillary and van der Waals forces cause nanotubes to deform or even collapse under metal contacts. Using ab-initio bandstructure calculations, we find that these deformations reduce the bandgap by as much as 30 \%, while fully collapsed nanotubes become metallic. Moreover degeneracy lifting, due to the broken axial symmetry and wavefunctions mismatch between the fully collapsed and the round portions of a CNT, leads to a three times higher contact resistance. The latter we demonstrate by contact resistance calculations within the tight-binding approach.    \item[PACS numbers]   "
1801_09483_kreuzer18.tex," Although frames, which are a generalization of bases, are important tools used in signal processing, their potential in other fields of acoustics has not been fully explored yet. Gabor frames are very well adapted to represent oscillating functions, and therefore have a great potential as ansatz functions for Helmholtz FEM/BEM. In this paper representations of the solution of a scattering problem in 2D using Gabor frames based on B-splines as building blocks are investigated and some properties of these frames will be shown based on numerical experiments. "
1801_07234_6dFGS.tex,"We present a new accurate catalog of narrow-line Seyfert 1 galaxies (NLS1s) in the southern hemisphere from the Six-degree Field Galaxy Survey (6dFGS) final data release, which is currently the most extensive spectroscopic survey available in the southern sky whose database has not yet been systematically explored. We classified 167 sources as NLS1s based on their optical spectral properties. We derived flux-calibrated spectra for the first time that the 6dFGS indeed does not provide. By analyzing these spectra, we obtained strong correlations between the monochromatic luminosity at 5100 $\AA$ and the luminosity of H$\beta$ and [O III]$\lambda$5007 lines. The estimated central black hole mass and Eddington ratio have an average value of $9.55 \times 10^{6 M_{Edd$ respectively, which is a typical value for NLS1s. In the sample, 23 (13.8$\%$) NLS1s were detected at radio frequencies, and 10 (6.0$\%$) of them are radio-loud. Our results confirmed that radio-loud sources tend to have higher redshift, more massive black hole, and higher radio and optical luminosity than radio-quiet sources.}"
1801_00742_main.tex,"  Population protocols are a well established model of distributed  computation by mobile finite-state agents with very limited  storage. A classical result establishes that population protocols  compute exactly predicates definable in Presburger arithmetic. We  initiate the study of the minimal amount of memory required to  compute a given predicate as a function of its size. We present  results on the predicates $x \geq n$ for $n\in \N$ , and more  generally on the predicates corresponding to systems of linear  inequalities. We show that they can be computed by protocols with  $O(\log n)$ states (or, more generally, logarithmic in the  coefficients of the predicate), and that, surprisingly, some  families of predicates can be computed by protocols with $O(\log\log  n)$ states. We give essentially matching lower bounds for the class  of 1-aware protocols. "
1801_04159_wikirank.tex, 00-abstract
1801_04084_abstract.tex," Whenever modern CPUs encounter a conditional branch for which the condition cannot be evaluated yet, they predict the likely branch target and speculatively execute code. Such pipelining is key to optimizing runtime performance and is incorporated in CPUs for more than 15 years. In this paper, to the best of our knowledge, we are the first to study the inner workings and the security implications of such speculative execution. We revisit the assumption that speculatively executed code leaves no traces in case it is not committed. We reveal several measurable side effects that allow adversaries to enumerate mapped memory pages and to read arbitrary memory---all using only speculated code that was never fully executed. To demonstrate the practicality of such attacks, we show how a user-space adversary can probe for kernel pages to reliably break kernel-level ASLR in Linux in under three seconds and reduce the Windows 10 KASLR entropy by 18~bits in less than a second. "
1801_01261_minxss_instrument_paper_final_resubmit2.tex," The {\it Miniature X-ray Solar Spectrometer} (MinXSS) CubeSat is the first solar science oriented CubeSat mission flown for the NASA Science Mission Directorate, with the main objective of measuring the solar soft X-ray (SXR) flux and a science goal of determining its influence on Earth's ionosphere and thermosphere. These observations can also be used to investigate solar quiescent, active region, and flare properties. The MinXSS X-ray instruments consist of a spectrometer, called X123, with a nominal 0.15 keV full-width-half-maximum (FWHM) resolution at 5.9 keV and a broadband X-ray photometer, called XP. Both instruments are designed to obtain measurements from 0.5 \,-- \it Geostationary Operational Environmental Satellite keV at a nominal time cadence of 10 seconds. A description of the MinXSS instruments, performance capabilities, and relation to the  (GOES) 0.1 \,-- \,0.8 nm flux are discussed in this article. Early MinXSS results demonstrate the capability to measure variations of the solar spectral SXR flux between 0.8 \,-- \,12 keV from at least GOES A5--M5 (5 $\times$ 10 $^{-8}$ \,-- \,5 $\times$ 10 $^{-5}$ W m $^{-2}$ ) levels and infer physical properties (temperature and emission measure) from the MinXSS data alone. Moreover, coronal elemental abundances can be inferred, specifically Fe, Ca, Si, Mg, S, Ar, and Ni, when there is sufficiently high count rate at each elemental spectral feature. Additionally, temperature response curves and emission measure loci demonstrate the MinXSS sensitivity to plasma emission at different temperatures. MinXSS observations coupled with those from other solar observatories can help address some of the most compelling questions in solar coronal physics. Finally, simultaneous observations by MinXSS and {\it Reuven Ramaty High Energy Solar Spectroscopic Imager} (RHESSI) can provide the most spectrally complete soft X-ray solar flare photon flux measurements to date. "
1801_09224_AutoTag.tex," %2.27 On-body devices are an intrinsic part of the Internet-of-Things (IoT) vision to provide human-centric services. These on-body IoT devices are largely embedded devices that lack a sophisticated user interface to facilitate traditional Pre-Shared Key based security protocols. Motivated by this real-world security vulnerability, this paper proposes SecureTag, a system designed to add defense in depth against active attacks by integrating physical layer (PHY) information with upper-layer protocols. The underpinning of SecureTag is a signal processing technique that extracts the peculiar propagation characteristics of creeping waves to discern on-body devices. Upon overhearing a suspicious transmission, SecureTag initiates a PHY-based challenge-response protocol to mitigate attacks. We implement our system on different commercial off-the-shelf (COTS) wearables and a smartphone. Extensive experiments are conducted in a lab, apartments, malls, and outdoor areas, involving 12 volunteer subjects of different age groups, to demonstrate the robustness of our system. Results show that our system can mitigate 96.13 \% of active attack attempts while triggering false alarms on merely 5.64 \% of legitimate traffic. "
1801_09989_CrMoW-Cu_Hpt.tex," Cu-refractory composites containing Cr, Mo or W were subjected to severe plastic deformation using room temperature high-pressure torsion (HPT). A lamellar microstructure developed in each of the composites at equivalent strains of $\sim$ 75. The refractory metals developed $hkl\\langle111\rangle$ fibre textures with a slight tilt to the tangential direction, as expected for pure-shear deformation of a body-centred cubic phase. This texture was stronger and more clearly defined in Mo and W than in Cr. By applying additional HPT deformation to these samples, perpendicular to the original shear strain, it was found that the lamellar structure of Cu30Mo70 and Cu20W80 (wt. \%) composites could be retained at high equivalent strains and the refractory layer thickness could be reduced to 20--50 \,nm in Cu20W80 and 10-20 \,nm in Cu30Mo70. Although neighbouring regions of the microstucture were aligned and there was evidence of local texture in both composites, the bulk texture of Cu30Mo70 became weaker during this second step of HPT deformation. This was attributed to the refractory metal lamellae being discontinuous and imperfectly aligned. This work shows that it is possible to form ultrafine composites of Cu-group VI refractory metals via high-pressure torsion, with namolamellar structures being possible where there is a sufficient volume fraction of Mo or W. "
1801_06136_merged.tex," % \small \baselineskip=9pt  Nonnegative matrix factorization (NMF) is one of the most frequently-used matrix factorization models in data analysis. A significant reason to the popularity of NMF is its interpretability and the `parts of whole' interpretation of its components. Recently, max-times, or subtropical, matrix factorization (SMF) has been introduced as an alternative model with equally interpretable `winner takes it all' interpretation. In this paper we propose a new mixed linear--tropical model, and a new algorithm, called Latitude, that combines NMF and SMF, being able to smoothly alternate between the two. In our model, the data is modeled using the latent factors and latent parameters that control whether the factors are interpreted as NMF or SMF features, or their mixtures. We present an algorithm for our novel matrix factorization. Our experiments show that our algorithm improves over both baselines, and can yield interpretable results that reveal more of the latent structure than either NMF or SMF alone.  \bigskip \noindent Keywords: matrix factorization; subtropical algebra; NMF %%% Local Variables: %%% mode: latex %%% TeX-master: ""paper"" %%% End:   "
1801_03445_oscillation_prl.tex," % Radio frequency devices are an essential tool for manipulating particle spins in a storage ring. This paper reports the first simultaneous measurement of the horizontal and vertical components of the polarization vector in a storage ring under the influence of a radio frequency (rf) solenoid. The experiments were performed at the Cooler Synchrotron COSY in Jlich using a vector polarized, bunched $0.97GeV/c$ deuteron beam. Using the new spin feedback system, we set the initial phase difference between the solenoid field and the precession of the polarization vector to a predefined value. The feedback system was then switched off, allowing the phase difference to change over time, and the solenoid was switched on to rotate the polarization vector. We observed an oscillation of the vertical polarization component and the phase difference. The oscillations can be described using an analytical model. The results of this experiment also apply to other rf devices with horizontal magnetic fields, such as Wien filters. The precise manipulation of particle spins in storage rings is a prerequisite for measuring the electric dipole moment (EDM) of charged particles. "
1801_08787_Mars.tex," 	Dust and sand motion are a common sight on Mars. Understanding the interaction of atmosphere and Martian soil is fundamental to describe the planet's weather, climate and surface morphology. 	 	We set up a wind tunnel to study the lift of a mixture between very fine sand and dust in a Mars simulant soil. The experiments were carried out under Martian gravity in a parabolic flight. The reduced gravity was provided by a centrifuge under external microgravity. The onset of saltation was measured for a fluid threshold shear velocity of 0.82 $\pm$ 0.04 m/s. This is considerably lower than found under Earth gravity. 	 	In addition to a reduction in weight, this low threshold can be attributed to gravity dependent cohesive forces within the sand bed, which drop by 2/3 under Martian gravity. The new threshold for saltation leads to a simulation of the annual dust cycle with a Mars GCM that is in agreement with observations. 	 "
1801_01195_JointSpectralMeasurement-JMO_tutorial_revised-final.tex," The ability to determine the joint spectral properties of photon pairs produced by the processes of spontaneous parametric downconversion (SPDC) and spontaneous four wave mixing (SFWM) is crucial for guaranteeing the usability of heralded single photons and polarization-entangled pairs for multi-photon protocols. In this paper, we compare six different techniques that yield either a characterization of the joint spectral intensity or of the closely-related purity of heralded single photons. These six techniques include: i) scanning monochromator measurements, ii) a variant of Fourier transform spectroscopy designed to extract the desired information exploiting a resource-optimized technique, iii) dispersive fibre spectroscopy, iv) stimulated-emission-based measurement, v) measurement of the second-order correlation function $g^{(2)}$ for one of the two photons, and vi) two-source Hong-Ou-Mandel interferometry. We discuss the relative performance of these techniques for the specific cases of a SPDC source designed to be factorable and SFWM sources of varying purity, and compare the techniques' relative advantages and disadvantages. "
1801_02729_ESD_v6.tex," Quantum entanglement, the essential resource for quantum information processing, has rich dynamics under different environments. Probing different entanglement dynamics typically requires exquisite control of complicated system-environment coupling in real experimental systems. Here, by a simple control of the effective solid-state spin bath in a diamond sample, we observe rich entanglement dynamics, including the conventional asymptotic decay as well as the entanglement sudden death, a term coined for the phenomenon of complete disappearance of entanglement after a short finite time interval. Furthermore, we observe counter-intuitive entanglement rebirth after its sudden death in the same diamond sample by tuning an experimental parameter, demonstrating that we can conveniently control the non-Markovianity of the system-environment coupling through a natural experimental knob. Further tuning of this experimental knob can make the entanglement dynamics completely coherent under the same environmental coupling. Probing of entanglement dynamics, apart from its fundamental interest, may find applications in quantum information processing through control of the environmental coupling. "
1801_09005_twopoint_calib.tex," Calibrating narrow field of view soccer cameras is challenging because there are very few field markings in the image. Unlike previous solutions, we propose a two-point method, which requires only two point correspondences given the prior knowledge of base location and orientation of a pan-tilt-zoom (PTZ) camera. We deploy this new calibration method to annotate pan-tilt-zoom data from soccer videos. The collected data are used as references for new images. We also propose a fast random forest method to predict pan-tilt angles without image-to-image feature matching, leading to an efficient calibration method for new images. We demonstrate our system on synthetic data and two real soccer datasets. Our two-point approach achieves superior performance over the state-of-the-art method.    "
1801_07190_arXiv_submit_20180326.tex,"\abstract{ The low-temperature-differential (LTD) Stirling heat engine technology constitutes one of the important sustainable energy technologies. The basic question of how the rotational motion of the LTD Stirling heat engine is maintained or lost based on the temperature difference is thus a practically and physically important problem that needs to be clearly understood. Here, we approach this problem by proposing and investigating a minimal nonlinear dynamic model of an LTD kinematic Stirling heat engine. Our model is described as a driven nonlinear pendulum where the motive force is the temperature difference. The rotational state and the stationary state of the engine are described as a stable limit cycle and a stable fixed point of the dynamical equations, respectively. These two states coexist under a sufficient temperature difference, whereas the stable limit cycle does not exist under a temperature difference that is too small. Using a nonlinear bifurcation analysis, we show that the disappearance of the stable limit cycle occurs via a homoclinic bifurcation, with the temperature difference being the bifurcation parameter.}"
1801_02320_main.tex," Wave dark matter ( $\psi$ DM), which satisfies the Schr \""odinger-Poisson equation, has recently attracted substantial attention as a possible dark matter candidate. Numerical simulations have in the past provided a powerful tool to explore this new territory of possibility. Despite their successes to reveal several key features of $\psi$ DM, further progress in simulations is limited, in that cosmological simulations so far can only address formation of halos below $11 M_\odot$ and substantially more massive halos have become computationally very challenging to obtain. For this reason, the present work adopts a different approach in assessing massive halos by constructing wave-halo solutions directly from the wave distribution function. This approach bears certain similarity with the analytical construction of particle-halo (cold dark matter model). Instead of many collisionless particles, one deals with one single wave that has many non-interacting eigenstates. The key ingredient in the wave-halo construction is the distribution function of the wave power, and we use several halos produced by structure formation simulations as templates to determine the wave distribution function. Among different models, we find the fermionic King model presents the best fits and we use it for our wave-halo construction. We have devised an iteration method for constructing the nonlinear halo, and demonstrate its stability by three-dimensional simulations. A Milky-Way-sized halo has also been constructed, and the inner halo is found flatter than the NFW profile. These wave-halos have small-scale interferences both in space and time producing time-dependent granules. While the spatial scale of granules varies little, the correlation time is found to increase with radius by one order of magnitude across the halo. % % \item[Usage] %Secondary publications and information retrieval purposes. % \item[PACS numbers] %May be entered using the \verb+ #1 + command. % \item[Structure] %You may use the description environment to structure your abstract; %use the optional argument of the \verb+ \item+ command to give the category of each item. % "
1801_09497_circ-pol-PLE.tex, % insert abstract here
1801_06446_ms.tex," The Leading Arm (LA) of the Magellanic Stream is a vast debris field of \hi \ clouds connecting the Milky Way and the Magellanic Clouds. It represents an example of active gas accretion onto the Galaxy. Previously only one chemical abundance measurement had been made in the LA. %(S/H)=35$\pm$ 7 \% solar reported toward the AGN NGC \,3783. Here we present chemical abundance measurements using  Hubble Space Telescope /Cosmic Origins Spectrograph %( HST /COS) and Green Bank Telescope spectra of four AGN sightlines passing through the LA and three nearby sightlines %passing through \hi \ high-velocity clouds (HVCs) that may trace outer fragments of the LA. %* We find low oxygen abundances, ranging from 4.0$^{+2.0}_{-2.0}$ \% solar to %* 12.6$^{+6.0}_{-4.1}$ \% solar, in the confirmed LA directions, with the lowest values found in the region known as LA III, farthest from the LMC. These abundances are substantially lower than the single previous measurement, S/H=35$\pm$ 7 \%{\bf Published tidal models indicate %this origin occurred $\approx$1.5--2.5\,Gyr ago during a close encounter %between the LMC and SMC.} solar (Lu et al. 1998), but are in agreement with those reported in the SMC filament of the trailing Stream, supporting a common origin in the SMC (not the LMC) for the majority of the LA and trailing Stream. This provides important constraints for models of the formation of the Magellanic System. %The abundances are too low for a LMC origin. %* % %We find no evidence for an LMC filament in the LA, as a counterpart %to the LMC filament in the Stream. % Is this too speculative? Detract from main focus on abundances %Our data also the spatial extent of the LA: Finally, two of the three nearby sightlines show high-velocity clouds with \hi \ columns, kinematics, and oxygen abundances consistent with LA membership. This suggests that the LA is larger than traditionally thought, extending at least 20 \degr \ further to the Galactic northwest. %covering $\approx60\degr\!\times\!80\degr$ . %but it's not rectangular... "
1801_06767_isom_emb_of_negative_k_V1.tex," We prove the existence of $C^{1,1}$ isometric immersions of several classes of metrics on surfaces $(\M,g)$ into the three-dimensional Euclidean space $\R^3$ , where the metrics $g$ have strictly negative curvature. These include the standard hyperbolic plane, generalised helicoid-type metrics and generalised Enneper metrics. Our proof is based on the method of compensated compactness and invariant regions in hyperbolic conservation laws, together with several observations on the geometric quantities (Gauss curvature, metric components etc.) of negatively curved surfaces.  "
1801_04738.tex," For a finite dimensional algebra $\Lambda$ and a non-negative integer $n$ , we characterize when the set $\tilt_n\Lambda$ of additive equivalence classes of tilting modules with projective dimension at most $n$ has a minimal (or equivalently, minimum) element. This generalize results of Happel-Unger. Moreover, for an $n$ -Gorenstein algebra $\Lambda$ with $n\geq 1$ , we construct a minimal element in $n\Lambda$ . As a result, we give equivalent conditions for a $k$ -Gorenstein algebra to be Iwanaga-Gorenstein. Moreover, for an $1$ -Gorenstein algebra $\Lambda$ and its factor algebra $\Gamma=\Lambda/(e)$ , we show that there is a bijection between $\tilt_1\Lambda$ and the set $\sttilt\Gamma$ of isomorphism classes of basic support $\tau$ -tilting $\Gamma$ -modules, where $e$ is an idempotent such that $e\Lambda $ is the additive generator of projective-injective $\Lambda$ -modules. "
1801_00618_ISSPD.tex," Bipedal robots are a prime example of systems which exhibit highly nonlinear dynamics, underactuation, and undergo complex dissipative impacts. % highly dissipative impacts. % % Robust controllers are needed for systems as complex as bipedal robots that exhibit highly nonlinear dynamics, underactuations and also undergo highly dissipative impacts. % This paper discusses methods used to overcome a wide variety of uncertainties, with the end result being stable bipedal walking. % a type of controllers, termed the input to state stabilizing (ISSing) controllers, that ensure robustness of these bipedal walking robots to a wide variety of uncertainties. % % We will establish that PD control laws render a robotic system ISS. This result is extended to bipedal robots by using the hybrid invariance conditions and the stability of the zero dynamics. This control implementation overcomes a wide variety of uncertainties except for one: phase based uncertainty. % % It will be shown that phase to state stabilizing controllers when coupled with PD control laws yields stable and robust walking in the humanoid robot DURUS. % A set of parameterized desired trajectories are obtained through a partial hybrid zero dynamics (PHZD) based reconstruction process. % % These desired trajectories are parameterized by a function of time from 0 to 1 resulting in one step in the robot. Repetition of this step culminates into a hybrid periodic orbit. % A set of parameterized output functions are obtained through a partial hybrid zero dynamics (PHZD) % based reconstruction process. Through the use of a linear feedback control law, these time parameterized % trajectories are then tracked in each of the joints on the robot, % resulting in walking. % % By using a linear feedback control law, these time parameterized trajectories are then tracked in each of the joints of the robot, resulting in walking. % The principal contribution of this paper is to establish sufficiency conditions for yielding input to state stable (ISS) hybrid periodic orbits, i.e., stable walking gaits under model-based and phase-based uncertainties. In particular, it will be shown formally that exponential input to state stabilization (e-ISS) of the continuous dynamics, and hybrid invariance conditions are enough to realize stable walking in the $23$ -DOF bipedal robot DURUS. % The principal contribution of this paper is to show that linear feedback laws and time based parameterization yield input to state stable (ISS) hybrid periodic orbits, i.e., stable walking gaits. % Mathematical formulations that follow will indeed show that the walking thus obtained is input to state stable. % % Formal results are shown (but not limited to) for PD control laws that render the continuous dynamics exponential input to state stable (e-ISS). % Linear feedback laws and time based parameterization is used input to state stabilize (ISS) hybrid periodic orbits This main result will be supported through successful and sustained walking of the bipedal robot DURUS in a laboratory environment. % This paper analyzes the input to state stability properties of controllers which stabilize hybrid periodic orbits. % Systems that are input to state stable tend to be robust to modeling and sensing uncertainties.  %Specifically, we will select a class of controllers called rapidly exponentially stabilizing control Lyapunov functions that stabilize bipedal robotic walking; typically modeled as hybrid periodic orbits. %We will show with simulation results that there exists controllers that input to state stabilizes the given stable periodic orbit; called the input to state stabilizing control Lyapunov functions.  % Given the existence of a control Lyapunov function, this paper shows that it is possible to obtain an input to state stable variant of this control Lyapunov function for affine hybrid systems; specifically bipedal walking robots. This control Lyapunov function can then be utilized to realize robust behaviors in bipedal robotic walking.  "
1801_06541_abstract.tex," 	 Software messaging frameworks help avoid errors and reduce engineering efforts in building distributed systems by (1) providing an interface definition language (IDL) to specify precisely the structure of the message (i.e., the message {\em schema} ), and (2) automatically generating the serialization and deserialization functions that transform user data structures into binary data for sending across the network and vice versa. Similarly, a hardware-accelerated system, which consists of host software and multiple FPGAs, could also benefit from a messaging framework to handle messages both between software and FPGA and also between different FPGAs. The key challenge for a hardware messaging framework is that it must be able to support large messages with complex schema while meeting critical constraints such as clock frequency, area, and throughput.  In this paper, we present HGum, a messaging framework for hardware accelerators that meets all the above requirements. HGum is able to generate high-performance and low-cost hardware logic by employing a novel design that algorithmically parses the message schema to perform serialization and deserialization. Our evaluation of HGum shows that it not only significantly reduces engineering efforts but also generates hardware with comparable quality to manual implementation. "
1801_06119_LaliotisRetardation_4v.tex," Spectroscopy is a unique experimental tool for measuring the fundamental Casimir-Polder interaction between excited state atoms, or other polarisable quantum objects, and a macroscopic surface. Spectroscopic measurements probe atoms at nanometric distances away from the surface where QED retardation is usually negligeable and the atom-surface interaction is proportional to the inverse cube of the separation distance, otherwise known as the van der Waals regime. Here we focus on selective reflection, one of the main spectroscopic probes of Casimir-Polder interactions. We calculate for the first time selective reflection spectra using the full, distance dependent, Casimir-Polder energy shift and linewidth. We demonstrate that retardation can have significant effects, in particular for experiments with low lying energy states. We also show that the effective probing depth of selective reflection spectroscopy depends on the transition linewidth. Our analysis allows us to calculate selective reflection spectra with composite surfaces, such as metasurfaces, dielectric stacks, or even bi-dimensional materials.      "
1801_08690_charge_reco_paper.tex,"\abstract{ Large-area PhotoMultiplier Tubes () allow to efficiently instrument Liquid Scintillator (LS) neutrino detectors, where large target masses are pivotal to compensate for neutrinos' extremely elusive nature. Depending on the detector light yield, several scintillation photons stemming from the same neutrino interaction are likely to hit a single in a few tens/hundreds of nanoseconds, resulting in several photoelectrons (s) to pile-up at the anode. In such scenario, the signal generated by each is entangled to the others, and an accurate charge reconstruction becomes challenging. This manuscript describes an experimental method able to address the charge reconstruction in the case of large pile-up, providing an unbiased charge estimator at the permille level up to 15 detected s. The method is based on a signal filtering technique (Wiener filter) which suppresses the noise due to both and readout electronics, and on a Fourier-based deconvolution able to minimize the influence of signal distortions ---such as an overshoot. The analysis of simulated waveforms shows that the slope of a linear regression modeling the relation between reconstructed and true charge values  improves from $0.769 \pm 0.001$ (without deconvolution) to $0.989 \pm 0.001$ (with deconvolution), where unitary slope implies perfect reconstruction. A C++ implementation of the charge reconstruction algorithm is available online at~website. }"
1801_02341_Pebbleisolation.tex,"The growth of a planetary core by pebble accretion stops at the so-called pebble isolation mass, when the core generates a pressure bump that traps drifting pebbles outside its orbit. The value of the pebble isolation mass is crucial in determining the final planet mass. If the isolation mass is very low, gas accretion is protracted and the planet remains at a few Earth masses with a mainly solid composition. For higher values of the pebble isolation mass, the planet might be able to accrete gas from the protoplanetary disc and grow into a gas giant. Previous works have determined a scaling of the pebble isolation mass with cube of the disc aspect ratio. Here, we expand on previous measurements and explore the dependency of the pebble isolation mass on all relevant parameters of the protoplanetary disc. We use 3D hydrodynamical simulations to measure the pebble isolation mass and derive a simple scaling law that captures the dependence on the local disc structure and the turbulent viscosity parameter $\alpha$. We find that small pebbles, coupled to the gas, with Stokes number $\tau_{\rm f<0.005$ can drift through the partial gap at pebble isolation mass. However, as the planetary mass increases, particles must be decreasingly smaller to penetrate the pressure bump. Turbulent diffusion of particles, however, can lead to an increase of the pebble isolation mass by a factor of two, depending on the strength of the background viscosity and on the pebble size. We finally explore the implications of the new scaling law of the pebble isolation mass on the formation of planetary systems by numerically integrating the growth and migration pathways of planets in evolving protoplanetary discs. Compared to models neglecting the dependence of the pebble isolation mass on the $\alpha$-viscosity, our models including this effect result in higher core masses for giant planets. These higher core masses are more similar to the core masses of the giant planets in the solar system. }"
1801_06636_AMP_on_CD_180119_ARXIV.tex," In this paper we study a new metric for comparing Betti numbers functions in bidimensional persistent homology, based on coherent matchings, i.e. families of matchings that vary in a continuous way. We prove some new results about this metric, including its stability. In particular, we show that the computation of this distance is strongly related to suitable filtering functions associated with lines of slope $1$ , so underlining the key role of these lines in the study of bidimensional persistence. In order to prove these results, we introduce and study the concepts of extended Pareto grid for a normal filtering function as well as of transport of a matching. As a by-product, we obtain a theoretical framework for managing the phenomenon of monodromy in 2D persistent homology. "
1801_00458_CuCrO11Dec.tex," The magnetic phase diagram of CuCrO $_2$ was studied with a novel method of simultaneous Cu NMR and electric polarization techniques with the primary goal of demonstrating that regardless of cooling history of the sample the magnetic phase with specific helmet-shaped NMR spectra associated with interplanar disorder possesses electric polarization. Our result unequivocally confirms the assumption of Sakhratov et al.  [Phys. Rev. B {94}, 094410 (2016)] that the high-field low-temperature phase is in fact a 3D-polar phase characterised by a 3D magnetic order with tensor order parameter. In comparison with the results obtained in pulsed fields, a modified phase diagram is introduced defining the upper boundary of the first-order transition from the 3D-spiral to the 3D-polar phase. "
1801_10434_egpaper_final.tex,"  In multi-view human body capture systems, the recovered 3D geometry or even the acquired imagery data can be heavily corrupted due to occlusions, noise, limited field-of-view, etc. Direct estimation of 3D pose, body shape or motion on these low-quality data has been traditionally challenging.In this paper, we present a graph-based non-rigid shape registration framework that can simultaneously recover 3D human body geometry and estimate pose/motion at high fidelity.Our approach first generates a global full-body template by registering all poses in the acquired motion sequence.We then construct a deformable graph by utilizing the rigid components in the global template.We directly warp the global template graph back to each motion frame in order to fill in missing geometry. Specifically,we combine local rigidity and temporal coherence constraints to maintain geometry and motion consistencies.Comprehensive experiments on various scenes show that our method is accurate and robust even in the presence of drastic motions. "
1801_02240.tex, \noindent A governing equation is derived for determining a) the class of deformed shapes and corresponding reference director distributions of a stress-free nematic glass membrane that has a prescribed spontaneous stretch field and b) the class of reference configurations and corresponding director distributions on it resulting in a stress-free given deformed shape of a nematic glass sheet with a prescribed spontaneous stretch field. The proposed solution rests on an understanding of how the Lagrangian dyad of a deformation of a membrane maps into the Eulerian dyad in three dimensional ambient space.
1801_03240_kirill01.tex," Localized electromagnetic modes and negligible Ohmic losses dictate the growing interest in subwavelength all-dielectric nanoparticles. Although an exhaustive volume of study dealt with interaction of all-dielectric nanostructures with free-space electromagnetic fields, their performance as integrated photonics elements remains untackled.	 We present an experimental study of optical coupling between a resonant subwavelength silicon nanodisk and a non-resonant silicon waveguide, as probed by third harmonic generation microscopy. By placing the nanodisks at different distances from the waveguide, we observe third harmonic intensity modulation by a factor of up to 4.5. %indicating strong modifications in the local field of the nanodisks by the waveguide. This modulation is assigned to changes in the local field enhancement within the nanodisks caused by their coupling to the waveguides and subsequent modulation of their magnetic-type resonances. Interestingly, although the waveguide presents an additional loss channel for the nanodisk, we observe an increase in the local field strength within the nanodisk, as verified by rigorous full-wave simulations. This work makes a step toward integration of all-dielectric nanoparticles on photonic chips.  	%The numerical simulations was carried out and it helps us to connect this THG signal changing with spectral shift of the resonance of the system. "
1801_07958.tex," An interesting proposal has recently been made to extend massive gravity models beyond dRGT by a disformal transformation of the metric. In this Letter we want to note that it can be viewed as a mimetic extension of dRGT gravity which enormously simplifies the Hamiltonian analysis. In particular, pure gravity sector is equivalent to the usual dRGT gravity coupled to a constrained scalar field. And we also give some comments about possible matter couplings. "
1801_02859_MF_Poisson.tex,"\abstract{The matched filter (MF) is widely used to detect signals hidden within the noise. If the noise is Gaussian, its performances are well-known and describable in an elegant analytical form. The treatment of non-Gaussian noises is often cumbersome as in most of the cases there is no analytical framework. This is true also for Poisson noise which, especially in the low-number count regime, presents the additional difficulty to be discrete. For this reason, in the past methods based on heuristic or semi-heuristic arguments have been proposed. Recently, an analytical form of the MF has been obtained but in the proposed implementation the computation of the probability of false detection or false alarm ($\PFA$) is based on numerical simulations. This is an inefficient and time consuming approach. Here, we present an effective method to compute the $\PFA$ based on the saddlepoint approximation which is fast, able to provide excellent results and easy to implement. Theoretical arguments are provided as well the results of some numerical experiments. }"
1801_10492_deep-predictive-models-preprint-v1.tex," Automatic music generation is a compelling task where much recent progress has been made with deep learning models. In this paper, we ask how these models can be integrated into interactive music systems; how can they encourage or enhance the music making of human users? Musical performance requires prediction to operate instruments, and perform in groups. We argue that predictive models could help interactive systems to understand their temporal context, and ensemble behaviour. Deep learning can allow data-driven models with a long memory of past states. We advocate for predictive musical interaction, where a predictive model is embedded in a musical interface, assisting users by predicting unknown states of musical processes. We propose a framework for incorporating such predictive models into the sensing, processing, and result architecture that is often used in musical interface design. We show that our framework accommodates deep generative models, as well as models for predicting gestural states, or other high-level musical information. We motivate the framework with two examples from our recent work, as well as systems from the literature, and suggest musical use-cases where prediction is a necessary component. "
1801_08140_sfsuppression.tex," Gas-rich minor mergers contribute significantly to the gas reservoir of early-type galaxies (ETGs) at low redshift, yet the star formation efficiency (SFE; the star formation rate divided by the molecular gas mass) appears to be strongly suppressed following some of these events, in contrast to the more well-known merger-driven starbursts. We present observations with the Atacama Large Millimeter/submillimeter Array (ALMA) of six ETGs, which have each recently undergone a gas-rich minor merger, as evidenced by their disturbed stellar morphologies. These galaxies were selected because they exhibit extremely low SFEs. We use the resolving power of ALMA to study the morphology and kinematics of the molecular gas. The majority of our galaxies exhibit spatial and kinematical irregularities, such as detached gas clouds, warps, and other asymmetries. These asymmetries support the interpretation that the suppression of the SFE is caused by dynamical effects stabilizing the gas against gravitational collapse. Through kinematic modelling we derive high velocity dispersions and Toomre $Q$ stability parameters for the gas, but caution that such measurements in edge-on galaxies suffer from degeneracies. We estimate merger ages to be about 100~Myr based on the observed disturbances in the gas distribution. Furthermore, we determine that these galaxies lie, on average, two orders of magnitude below the Kennicutt-Schmidt relation for star-forming galaxies as well as below the relation for relaxed ETGs. We discuss potential dynamical processes responsible for this strong suppression of star formation surface density at fixed molecular gas surface density.  "
1801_01613_sect1.tex," %The performance of wireless multicast is bottlenecked by the user with the worst channel condition. Hence, a fundamental challenge in wireless multicast has been how to simultaneously achieve high-throughput and low-delay for reliably serving a large number of users. In this paper, we propose a multicast coding scheme called Multi-Channel Moving Window Codes (MC-MWC), which can simultaneously realize the following three benefits: (i) High throughput : We show that in the many-user many-channel asymptotic regime, to achieve a non-vanishing throughput, the multi-channel resources required by MC-MWC achieves an algorithm independent lower bound in an order sense. On the other hand, the multi-channel resource required by a conventional scheme based on optimal static channel allocation and capacity-achieving codes is shown to be doubly-exponentially larger than that required by MC-MWC. (ii) Low delay : Using large deviations theory, we show that the delay of MC-MWC decreases linearly as the number of channels grows, while the delay reduction of conventional schemes is bounded by a constant factor. (iii) Constant feedback overhead : The feedback overhead of MC-MWC is a constant that is independent of both the number of receivers in each session and the number of sessions in the network. Trace-driven simulation and numerical results are provided to demonstrate these benefits.  %A major challenge of wireless multicast is to be able to simultaneously achieve high throughput and low delay for a large number of users. In this paper, we consider a multi-channel, multi-session wireless multicast network and investigate how much throughput and delay gains we can harness from exploiting the multi-channel resources. We propose and analyze a scheme called Multi-Channel Moving Window Codes (MC-MWC). First, we show that in the many-user many-channel asymptotic regime, to achieve any non-vanishing throughput, the multi-channel resources required by MC-MWC achieves an algorithm independent lower bound in an order sense. Hence, MC-MWC achieves order-optimal throughput in the many-user many-channel asymptotic regime. On the other hand, the multi-channel resource required by a conventional scheme based on the optimal static channel allocation and capacity-achieving codes is shown to be doubly-exponentially larger than that required by MC-MWC. Second, using large deviations theory, we show that the delay of MC-MWC decreases linearly as the number of channels grows, while the delay reduction of conventional schemes is no more than a finite constant. Third, the feedback overhead of MC-MWC is a constant that is independent of both the number of receivers in each session and the number of sessions in the network. %Finally, our trace-driven simulation and numerical results validate the analytical results and show that the implementation complexity of MC-MWC is affordable in practice. A fundamental challenge in wireless multicast has been how to simultaneously achieve high-throughput and low-delay for reliably serving a large number of users. In this paper, we show how to harness substantial throughput and delay gains by exploiting multi-channel resources. We develop a new scheme called Multi-Channel Moving Window Codes (MC-MWC) for multi-channel multi-session wireless multicast. The salient features of MC-MWC are three-fold. (i) High throughput: we show that MC-MWC achieves order-optimal throughput in the many-user many-channel asymptotic regime. Moreover, the number of channels required by a conventional channel-allocation based scheme is shown to be doubly-exponentially larger than that required by MC-MWC. (ii) Low delay: using large deviations theory, we show that the delay of MC-MWC decreases linearly with the number of channels, while the delay reduction of conventional schemes is no more than a finite constant. (iii) Low feedback overhead: the feedback overhead of MC-MWC is a constant that is independent of both the number of receivers in each session and the number of sessions in the network. Finally, our trace-driven simulation and numerical results validate the analytical results and show that the implementation complexity of MC-MWC is low. "
1801_00235_sample-sigconf.tex," Crossfire attack is a recently proposed threat designed to disconnect whole geographical areas, such as cities or states, from the Internet. Orchestrated in multiple phases, the attack uses a massively distributed botnet to generate low-rate benign traffic aiming to congest selected network links, so-called target links. The adoption of benign traffic, while simultaneously targeting multiple network links, makes the detection of the Crossfire attack a serious challenge. In this paper, we propose a framework for early detection of Crossfire attack, i.e., detection in the warm-up period of the attack. We propose to monitor traffic at the potential decoy servers and discuss the advantages comparing with other monitoring approaches. Since the low-rate attack traffic is very difficult to distinguish from the background traffic, we investigate several deep learning methods to mine the spatiotemporal features for attack detection.   % In this paper we simulated the crossfire attack and investigated the effectiveness of % different Machine Learning approaches % to detect the attack during its warm-up period (i.e., the time period before the actual attack % takes place) so that preventive measures can be taken before the actual attack. % Since the occurrence of an attack is rare, % we have set the instances of abnormal behaviour in our simulation to be low (i.e., 6 \%). %The problem of detecting a Crossfire attack can be modeled as an unsupervised anomaly detection problem or a supervised time-series classification problem.  We investigate Autoencoder, Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) Network to detect the Crossfire attack during its warm-up period. We report encouraging experiment results. % and show that they achieve significantly better performance in terms of F1 score than %the state-of-the-art anomaly detection method, namely the Isolation Forest. "
1801_03014.tex," We consider a Neumann problem for strictly convex variational functionals of linear growth. We establish the existence of minimisers among $1,1$ -functions provided that the domain under consideration is simply connected. Hence, in this situation, the relaxation of the functional to the space of functions of bounded variation, which has better compactness properties, is not necessary. Similar $1,1$ -regularity results for the corresponding Dirichlet problem are only known under rather restrictive convexity assumptions limiting its non-uniformity up to the borderline case of the minimal surface functional, whereas for the Neumann problem no such quantified version of strong convexity is required. "
1801_01118_paper.tex,"   Temporal inhomogeneities in event sequences of natural and social phenomena have been characterized in terms of interevent times and correlations between interevent times. The inhomogeneities of interevent times have been extensively studied, while the correlations between interevent times, often called correlated bursts, are largely unexplored. For measuring the correlated bursts, two relevant approaches were suggested, i.e., memory coefficient and burst size distribution. Empirical analyses have revealed that the larger memory coefficient tends to be associated with the heavier tail of burst size distribution. In particular, empirical findings in human activities appear inconsistent, such that the memory coefficient is close to $0$ , while burst size distributions follow a power law. In order to comprehend these observations, we derive the analytical form of the memory coefficient as a function of parameters describing interevent time and burst size distributions. Our analytical result can explain the general tendency of the larger memory coefficient being associated with the heavier tail of burst size distribution. We also find that the apparently inconsistent observations in human activities are compatible with each other, indicating that the memory coefficient has limits to measure the correlated bursts. "
1801_05978_PRA_vectorbeams.tex," We report experiments on propagation of scalar and vector optical beams through random phase screens mimicking turbulence and show that the intensity profile of the beam containing a C-point polarization singularity shows maximally robust behavior. This observation is explained in terms of the polarization and orbital angular momentum (OAM) diversity in the beam. The $l = 0$ and $l = 1$ OAM states whose vector combination leads to the C-point singularity are seen to produce complementary speckle intensity patterns with significant negative correlation on propagation through a random phase screen. This unique property of C-point singularity makes it superior to other inhomogeneous polarization states as demonstrated in our experiments. The results provide an important generic guideline for designing beams that can maintain optimally robust beam intensity profile on passing through random phase fluctuations and are expected to have a number of applications.  {\bf Keywords} : Polarization singularities, orbital angular momentum (OAM) states of light, propagation through turbulence, singular optics  "
1801_01708_nbf_arxiv.tex," We introduce negative binomial matrix factorization~(NBMF), a matrix factorization technique specially designed for analyzing over-dispersed count data. It can be viewed as an extension of Poisson matrix factorization (PF) perturbed by a multiplicative term which models exposure. This term brings a degree of freedom for controlling the dispersion, making NBMF more robust to outliers. We show that NBMF allows to skip traditional pre-processing stages, such as binarization, which lead to loss of information. Two estimation approaches are presented: maximum likelihood and variational Bayes inference. We test our model with a recommendation task and show its ability to predict user tastes with better precision than PF. "
1801_05671_2018_Nguyen_HRI_physical_hri_controller.tex,"  % With robots leaving factories and entering less controlled domains, possibly sharing the space with humans, safety is paramount and multimodal awareness of the body surface and the surrounding environment is fundamental. Taking inspiration from peripersonal space representations in humans, we present a framework on a humanoid robot that dynamically maintains such a protective safety zone, composed of the following main components: (i) a human 2D keypoints estimation pipeline employing a deep learning based algorithm, extended here into 3D using disparity; (ii) a distributed peripersonal space representation around the robot's body parts; (iii) a reaching controller that incorporates all obstacles entering the robot's safety zone on the fly into the task. Pilot experiments demonstrate that an effective safety margin between the robot's and the human's body parts is kept. The proposed solution is flexible and versatile since the safety zone around individual robot and human body parts can be selectively modulated---here we demonstrate stronger avoidance of the human head compared to rest of the body. Our system works in real time and is self-contained, with no external sensory equipment and use of onboard cameras only. "
1801_09808_paper.tex," Linear approximations to the decision boundary of a complex model have become one of the most popular tools for interpreting predictions. In this paper, we study such linear explanations produced either post-hoc by a few recent methods or generated along with predictions with contextual explanation networks (CENs). We focus on two questions: (i) whether linear explanations are always consistent or can be misleading, and (ii) when integrated into the prediction process, whether and how explanations affect performance of the model. Our analysis sheds more light on certain properties of explanations produced by different methods and suggests that learning models that explain and predict jointly is often advantageous. "
1801_03999_CompleteManuscriptFile_v3.tex,"  Substantial effort has been devoted in determining the ideal proxy for quantifying the morphology of the hot intracluster medium in clusters of galaxies. These proxies, based on X-ray emission, typically require expensive, high-quality X-ray observations making them difficult to apply to large surveys of groups and clusters. Here, we compare optical relaxation proxies with X-ray asymmetries and centroid shifts for a sample of SDSS clusters with high-quality, archival X-ray data from Chandra and XMM-Newton . The three optical relaxation measures considered are: the shape of the member-galaxy projected velocity distribution -- measured by the Anderson-Darling (AD) statistic, the stellar mass gap between the most-massive and second-most-massive cluster galaxy, and the offset between the most-massive galaxy (MMG) position and the luminosity-weighted cluster centre. The AD statistic and stellar mass gap correlate significantly with X-ray relaxation proxies, with the AD statistic being the stronger correlator. Conversely, we find no evidence for a correlation between X-ray asymmetry or centroid shift and the MMG offset. High-mass clusters ( $M_halo > 10^{14.5}\Msun$ ) in this sample have X-ray asymmetries, centroid shifts, and Anderson-Darling statistics which are systematically larger than for low-mass systems. Finally, considering the dichotomy of Gaussian and non-Gaussian clusters (measured by the AD test), we show that the probability of being a non-Gaussian cluster correlates significantly with X-ray asymmetry but only shows a marginal correlation with centroid shift. These results confirm the shape of the radial velocity distribution as a useful proxy for cluster relaxation, which can then be applied to large redshift surveys lacking extensive X-ray coverage. "
1801_01586_AutoencoderReview.tex," Many of the existing machine learning algorithms, both supervised and unsupervised, depend on the quality of the input characteristics to generate a good model. The amount of these variables is also important, since performance tends to decline as the input dimensionality increases, hence the interest in using feature fusion techniques, able to produce feature sets that are more compact and higher level. A plethora of procedures to fuse original variables for producing new ones has been developed in the past decades. The most basic ones use linear combinations of the original variables, such as PCA ( Principal Component Analysis ) and LDA ( Linear Discriminant Analysis ), while others find manifold embeddings of lower dimensionality based on non-linear combinations, such as Isomap or LLE ( Linear Locally Embedding ) techniques.  More recently, autoencoders (AEs) have emerged as an alternative to manifold learning for conducting nonlinear feature fusion. Dozens of AE models have been proposed lately, each with its own specific traits. Although many of them can be used to generate reduced feature sets through the fusion of the original ones, there also AEs designed with other applications in mind. The goal of this paper is to provide the reader with a broad view of what an AE is, how they are used for feature fusion, a taxonomy gathering a broad range of models, and how they relate to other classical techniques. In addition, a set of didactic guidelines on how to choose the proper AE for a given task is supplied, together with a discussion of the software tools available. Finally, two case studies illustrate the usage of AEs with datasets of handwritten digits and breast cancer. "
1801_01047_paper1.tex," Let $H$ be a graph on $h$ vertices. The number of induced copies of $H$ in a graph $G$ is denoted by $i_H(G)$ . Let $i_H(n)$ denote the maximum of $i_H(G)$ taken over all graphs $G$ with $n$ vertices. Let $f(n,h) = i^h a_i$ where $i=1^h a_i = n$ and the $a_i$ are as equal as possible. Let $g(n,h) = f(n,h) + i=1^h g(a_i,h)$ . It is proved that for almost all graphs $H$ on $h$ vertices it holds that $i_H(n)=g(n,h)$ for all $n \sqrt{h}$ . More precisely, we define an explicit graph property ${\cal P}_h$ which, when satisfied by $H$ , guarantees that $i_H(n)=g(n,h)$ for all $n \sqrt{h}$ . It is proved, in particular, that a random graph on $h$ vertices satisfies ${\cal P}_h$ with probability $1-o_h(1)$ . Furthermore, all extremal $n$ -vertex graphs yielding $i_H(n)$ in the aforementioned range are determined. We also prove a stability result. For $H \cal P_h$ and a graph $G$ with $n \sqrt{h}$ vertices satisfying $i_H(G) \ge f(n,h)$ , it must be that $G$ is obtained from a balanced blowup of $H$ by adding some edges inside the blowup parts. The {\em inducibility} of $H$ is $i_H = n \rightarrow \infty i_H(n)/n{h}$ . It is known that $i_H \ge h!/(h^h-h)$ for all graphs $H$ and that a random graph $H$ satisfies almost surely that $i_H 3\log hh!/(h^h-h)$ . We improve upon this upper bound almost matching the lower bound. It is shown that a graph $H$ which satisfies ${\cal P}_h$ has $i_H =(1+O(h^{-h^{1/3}}))h!/(h^h-h)$ . "
1801_08062_paper.tex, In this article we summarise our results from numerical simulations of $N=1$ supersymmetric Yang-Mills theory with gauge group SU(3). We use the formulation of Curci and Veneziano with clover-improved Wilson fermions. The masses of various bound states have been obtained at different values of the gluino mass and gauge coupling. Extrapolations to the limit of vanishing gluino mass indicate that the bound states form mass-degenerate supermultiplets.
1801_04784.tex,"  For $G$ a split semi-simple group scheme and $P$ a principal  $G$ -bundle on a relative curve $X\to S$ , we study a natural  obstruction for the triviality of $P$ on the complement of a  relatively ample Cartier divisor $D \subset X$ . We show, by  constructing explicit examples, that the obstruction is nontrivial  if $G$ is not simply connected but it can be made to vanish, if $S$ is the spectrum of a dvr (and some other hypotheses),  by a faithfully flat base change. The vanishing of  this obstruction is shown to be a sufficient condition for etale  local triviality if $S$ is a smooth curve, and the singular locus of  $X-D$ is finite over $S$ . "
1801_07530_cbms-master.tex," This paper contains an overview of background from stable homotopy theory used by Freed--Hopkins in their work on invertible extended topological field theories. We provide a working guide to the stable homotopy category, to the Steenrod algebra and to computations using the Adams spectral sequence. Many examples are worked out in detail to illustrate the techniques. "
abstract.tex," Large volumes of videos are continuously recorded from cameras deployed for traffic control and surveillance with the goal of answering ``after the fact'' queries: {\em identify video frames with objects of certain classes (cars, bags)} from many days of recorded video. While advancements in convolutional neural networks ( \name have enabled answering such queries with high accuracy, they are too expensive and slow. We build , a system for low-latency and low-cost querying on large video datasets. {\name} uses cheap ingestion techniques to index the videos by the objects occurring in them. At ingest-time, it uses compression and video-specific specialization of \cnns. {\name handles} the lower accuracy of the cheap \cnns by judiciously leveraging expensive \name at query-time. To reduce query time latency, we cluster similar objects and hence avoid redundant processing. Using experiments on video streams from traffic, surveillance and news channels, we see that  uses $58\times$ fewer GPU cycles than running expensive ingest processors and is $37\times$ faster than processing all the video at query time. "
abstract.tex," Online Reputation Monitoring (ORM) is concerned with the use of computational tools to measure the reputation of entities online, such as politicians or companies. In practice, current ORM methods are constrained to the generation of data analytics reports, which aggregate statistics of popularity and sentiment on social media. We argue that this format is too restrictive as end users often like to have the flexibility to search for entity-centric information that is not available in predefined charts.  As such, we propose the inclusion of entity retrieval capabilities as a first step towards the extension of current ORM capabilities. However, an entity's reputation is also influenced by the entity's relationships with other entities. Therefore, we address the problem of Entity-Relationship (E-R) retrieval in which the goal is to search for multiple connected entities. This is a challenging problem which traditional entity search systems cannot cope with. Besides E-R retrieval we also believe ORM would benefit of text-based entity-centric prediction capabilities, such as predicting entity popularity on social media based on news events or the outcome of political surveys. However, none of these tasks can provide useful results if there is no effective entity disambiguation and sentiment analysis tailored to the context of ORM.  Consequently, this thesis address two computational problems in Online Reputation Monitoring: Entity Retrieval and Text Mining. We researched and developed methods to extract, retrieve and predict entity-centric information spread across the Web.  We proposed a new probabilistic modeling of the problem of E-R retrieval together with two fusion-based design patterns for creating representations of both entities and relationships. Furthermore, we propose the Entity-Relationship Dependence Model, a novel early-fusion supervised model based on the Markov Random Field framework for Retrieval. Together with a new semi-automatic method to create test collections for E-R retrieval, we released a new test collection for that purpose that will foster research in this area. We performed experiments at scale with results showing that it is possible to perform E-R retrieval without using fix and pre-defined entity and relationship types, enabling a wide range of queries to be addressed. We tackled Entity Filtering and Financial Sentiment Analysis using a supervised learning approach and studied several possible features for that purpose. We participated in two well known external competitions on both tasks, obtaining state-of-the-art performance. Moreover, we performed analysis of the predictive power of a wide set of signals extracted from online news to predict the popularity of entities on Twitter. We also studied several sentiment aggregate functions on Twitter to study the feasibility of using entity-centric sentiment on social media to predict political opinion polls.  Finally, we created and released an adaptable Entity Retrieval and Text Mining framework that puts together all the building blocks necessary to perform ORM and can be reused in multiple application scenarios, from computational journalism to politics and finance. This framework is able to collect texts from online media, identify entities of interest, perform entity and E-R retrieval as well as classify sentiment polarity and intensity. It supports multiple data aggregation methods together with visualization and modeling techniques that can be used for both descriptive and predictive analytics. "
abstract.tex," \noindent We construct a \qcategorically \ enhanced Grothendieck six-functor formalism on schemes of finite type over the complex numbers. In addition to satisfying many of the same properties as M.~Saito's derived categories of mixed Hodge modules, this new six-functor formalism receives canonical motivic realization functors compatible with Grothendieck's six functors on constructible objects. "
MLB-draft-1_26_18.tex," 	Systemic and idiosyncratic patterns in pitching mechanics of 24 top starting pitchers in Major League Baseball (MLB) are extracted and discovered from PITCHf/x database. These evolving patterns across different pitchers or seasons are represented through three exclusively developed graphic displays. Understanding on such patterned evolutions will be beneficial for pitchers' wellbeing in signaling potential injury, and will be critical for expert knowledge in comparing pitchers. Based on data-driven computing, a universal composition of patterns is identified on all pitchers' mutual conditional entropy matrices. The first graphic display reveals that this universality accommodates physical laws as well as systemic characteristics of pitching mechanics. Such visible characters point to large scale factors for differentiating between distinct clusters of pitchers, and simultaneously lead to detailed factors for comparing individual pitchers. The second graphic display shows choices of features that are able to express a pitcher's season-by-season pitching contents via a series of 3(+2)D point-cloud geometries. The third graphic display exhibits exquisitely a pitcher's idiosyncratic pattern-information of pitching across seasons by demonstrating all his pitch-subtype evolutions. These heatmap-based graphic displays are platforms for visualizing and understanding pitching mechanics. "
Information-tree-datamatrix1_25_18.tex," Phylogenetic trees in genetics and biology in general are all binary. We make an attempt to answer one fundamental question: Is such binary branching from the coarsest to the finest scales sustained by data? We convert this question into an equivalent one: where is the structural information of tree in a data matrix? Results from this conceptual as well as computing issue afford us to conclude a negative answer: Each branch being split into two at each inter-node of tree from the top to bottom levels is a man-made structure. The data-driven computing paradigm Data Mechanics is employed here to reveal that information of tree is composed of a set of selected temperatures (or scales), each of which has a clustering composition strictly regulated by a temperature-specific cluster-sharing probability matrix. The resultant Data Cloud Geometry (DCG) tree on the space of species is proposed as the authentic structure contained in data. Particularly each core clusters on the finest scale, the bottom level, of DCG tree should not be further partitioned because of uniformity. Beyond the finest scale, the branching of DCG tree is primarily based on probability, which induces an Ultrametric satisfying super triangular inequality property. This Ultrametric property differentiates DCG tree from all popular trees based on Hierarchical clustering (HC) algorithm, which typically employs an empirical, often ad hoc distance measure. Since this measure is regulated by the triangular inequality, it is not capable of producing a ``flat"" branch, in which all its members (more than two) have equal distances to each others. We demonstrate such information content on an illustrative zoo data first, and then on two genomic data. 	"
abstract.tex," This paper revisits NDN deployment in the IoT with a special focus on the interaction of sensors {\em and actuators} . Such scenarios require high responsiveness and limited control state at the constrained nodes. We argue that the NDN request-response pattern which prevents data push is vital for IoT networks. We contribute HoP-and-Pull (HoPP), a robust publish-subscribe scheme for typical IoT scenarios that targets IoT networks consisting of hundreds of resource constrained devices at intermittent connectivity. Our approach limits the FIB tables to a minimum and naturally supports mobility, temporary network partitioning, data aggregation and near real-time reactivity. We experimentally evaluate the protocol in a real-world deployment using the IoT-Lab testbed with varying numbers of constrained devices, each wirelessly interconnected via IEEE 802.15.4 LowPANs. Implementations are built on CCN-lite with RIOT and support experiments using various single- and multi-hop scenarios. "
